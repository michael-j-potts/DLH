{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b66248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general instructions\n",
    "\n",
    "#2 or more patient encounters are required as the authors remove the last encounter while making predictions\n",
    "# patient mortality is the outcome label and aquired from mimic. \n",
    "#2825 out of 7537 patients died (37.9%)\n",
    "#Data set was divided into  train (5275/7537), validation (753/7537) and test (1509/7537)\n",
    "# Once the optimal parameters were selected, the model was retrained by combining train and validation (6028/7537)\n",
    "#\n",
    "#\n",
    "#********************** stars mean the data is not correct but the implemention functions.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b683be-1b9a-4270-952b-a8e5a9813fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from __future__ import division\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c5b59-3e7a-4490-aa61-708d22d07470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed - only really necessary to compare results with each other, but can remove from the submission\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = \"mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "\n",
    "# define data - separated to show which modules load (some take much longer than others)\n",
    "#The commented out sections take a long time to load, and weren't immidiately useful at this stage\n",
    "#Can uncommment when relevant\n",
    "\n",
    "#TODO\n",
    "# - reorganize into a definition() after  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients = pd.read_csv(DATA_PATH + '/PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef884c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnosis = pd.read_csv(DATA_PATH + '/DIAGNOSES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Procedures = pd.read_csv(DATA_PATH + '/D_ICD_PROCEDURES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba977e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Prescriptions = pd.read_csv(DATA_PATH + '/PRESCRIPTIONS.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef15cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabEvents = pd.read_csv(DATA_PATH + '/LABEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabLabels = pd.read_csv(DATA_PATH + '/D_LABITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Admissions = pd.read_csv(DATA_PATH + '/ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcedureLabels = pd.read_csv(DATA_PATH + '/D_ITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea694043",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcuStays = pd.read_csv(DATA_PATH + '/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChartEvents = pd.read_csv(DATA_PATH + '/CHARTEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient inclusion - Only ICU patients with 2 or more visits are included. Patient IDs gathered\n",
    "#May be incorrect as the paper specifies only 7,537 patients in their list.\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#********************************************\n",
    "#Generate an ignore list for later ease of use.\n",
    "#Manually verified that all the included patients have 2 or more subject ids. Refine by HADM_ID? ICUSTAY_ID?\n",
    "\n",
    "def Patient_Inclusion(IcuStays):\n",
    "    IcuList = IcuStays.to_numpy()\n",
    "\n",
    "    unique, counts = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    print(\"Max encouters should equal 42?\", \"max counts: \", max(counts))\n",
    "    \n",
    "    AllPatients = np.array(list(zip(np.transpose(unique), np.transpose(counts))))\n",
    "    \n",
    "    PatientList = []\n",
    "    IgnoreList = []\n",
    "    \n",
    "    for i in AllPatients:\n",
    "        if i[1] >= 2:\n",
    "            PatientList.append(i)\n",
    "        else:\n",
    "            IgnoreList.append(i[0])    \n",
    "\n",
    "    #print(len(IgnoreList))\n",
    "    return PatientList, IgnoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to create sequence lists like for Visit ID. \n",
    "\n",
    "def CreateList(n):\n",
    "    lst = []\n",
    "    for i in range(n):\n",
    "        lst.append(i)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b177839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be added to main at the end\n",
    "\n",
    "#separate patient ids and count information\n",
    "PatientList, IgnoreList = Patient_Inclusion(IcuStays)\n",
    "PatientList = pd.DataFrame(PatientList, columns = ['ID', 'Count'])\n",
    "Patient_ID = PatientList['ID']\n",
    "Visits = PatientList['Count']\n",
    "VisitID = [] * len(Visits)\n",
    "count = 0\n",
    "for i in Visits:\n",
    "    temp = CreateList(i)\n",
    "    VisitID.append(temp)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021b663-956f-47c5-87dc-ec28f2029ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data statistics - Ethnicity and visit statistics\n",
    "\n",
    "def Ethnicity_Statistics(Admissions):\n",
    "    white = 0\n",
    "    black = 0\n",
    "    hispanic = 0\n",
    "    asian = 0\n",
    "    other = 0\n",
    "    \n",
    "    #separate out ethnicity information\n",
    "    EthStatistics = Admissions[['SUBJECT_ID', 'ETHNICITY']]\n",
    "    IcuPatientEth = EthStatistics[EthStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "\n",
    "    #Not yet working.... General idea.\n",
    "    for i in IcuPatientEth.ETHNICITY:\n",
    "        if \"WHITE\" in i:\n",
    "            white += 1\n",
    "        if \"BLACK\" in i:\n",
    "            black += 1\n",
    "        if \"HISPANIC\" in i:\n",
    "            hispanic += 1\n",
    "        if \"ASIAN\"  in i:\n",
    "            asian += 1\n",
    "    \n",
    "        #I tried combining it, but the count was always more wrong...\n",
    "        if \"OTHER\" in i:\n",
    "            other += 1\n",
    "        if \"DECLINED\" in i:\n",
    "            other += 1\n",
    "        if \"NATIVE\" in i:\n",
    "            other += 1\n",
    "        if \"MIDDLE EASTERN\" in i:\n",
    "            other += 1\n",
    "        if \"MULTI\" in i:\n",
    "            other += 1\n",
    "        if \"UNABLE\" in i:\n",
    "            other += 1\n",
    "        if \"UNKNOWN\" in i:\n",
    "            other += 1\n",
    "        \n",
    "\n",
    "    print(np.round(white / len(IcuPatientEth), 3), \"percent white\")\n",
    "    print(np.round(black / len(IcuPatientEth), 3), \"percent black\")\n",
    "    print(np.round(hispanic / len(IcuPatientEth), 3), \"percent hispanic\")\n",
    "    print(np.round(asian / len(IcuPatientEth), 3), \"percent asian\")\n",
    "    print(np.round(other / len(IcuPatientEth), 3), \"percent other\")\n",
    "\n",
    "    print((np.round((white + black + hispanic + asian + other) / len(IcuPatientEth), 2)), \"percent total\")\n",
    "    \n",
    "    #*****************************************************\n",
    "    #6 too many values. Need to sort that out.... maybe? it doesnt actually really affect anything...\n",
    "    print(len(IcuPatientEth))\n",
    "    print(white + black + asian + hispanic + other)\n",
    "    return white, black, hispanic, asian, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be added to main at the end\n",
    "Ethnicity_Statistics(Admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50730d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age statistics - Date of birth DOB is in patients, and admissions time is in admissions.\n",
    "# need to correlate both to find patients true ages\n",
    "#Sorting to ensure that each patient id is matched in case of misordering\n",
    "def Age_statistics(Patients):\n",
    "\n",
    "    #separate out date of birth information and sort\n",
    "    AgeStatistics = Patients[['SUBJECT_ID', 'DOB']]\n",
    "    IcuPatientDob = AgeStatistics[AgeStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientDob = IcuPatientDob.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #separate out admission date and sort\n",
    "    AdmStatistics = Admissions[['SUBJECT_ID', 'ADMITTIME']]\n",
    "    IcuPatientAdm = AdmStatistics[AdmStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientAdm = IcuPatientAdm.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #minus dates to find ages\n",
    "    #********************************************************\n",
    "    # not done yet. need to convert at least part of the date to an int for each dataframe\n",
    "    #Year and month should be sufficient, but year month date would increase accuracy. remove time.\n",
    "    \n",
    "    \n",
    "Age_statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b781bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex statistics\n",
    "def Gender_Statistics(Patients):\n",
    "    male = 0\n",
    "    female = 0\n",
    "    \n",
    "    SexStatistics = Patients[['SUBJECT_ID', 'GENDER']]\n",
    "    IcuPatientSex = SexStatistics[SexStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    \n",
    "    for i in IcuPatientSex.GENDER:\n",
    "        if i == \"M\":\n",
    "            male += 1\n",
    "        if i == \"F\":\n",
    "            female += 1\n",
    "            \n",
    "    print(\"Male patients: \", male)\n",
    "    print(\"Female patients: \", female)\n",
    "    \n",
    "    print(np.round(male / len(IcuPatientSex), 2), \" percent male\")\n",
    "    print(np.round(female / len(IcuPatientSex), 2), \"percent female\")\n",
    "    \n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_Statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICD Code statistics\n",
    "def Icd_Codes(Diagnosis):\n",
    "    \n",
    "    #Isolate just the ICD codes and sequence of ICD codes\n",
    "    IcdStatistics = Diagnosis[['SUBJECT_ID', 'ICD9_CODE', 'SEQ_NUM']]\n",
    "    IcdStatistics.SEQ_NUM = IcdStatistics.SEQ_NUM.dropna()\n",
    "    IcuPatientIcd = IcdStatistics[IcdStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    \n",
    "    #To calculate the total unique ICD codes present\n",
    "    TotalIcdCode = IcuPatientIcd.filter([\"ICD9_CODE\"]).drop_duplicates()\n",
    "\n",
    "    #For ICD code groupings by patient id\n",
    "    PatientIcdCodes = IcuPatientIcd.groupby(\"SUBJECT_ID\")[\"ICD9_CODE\"].apply(list)\n",
    "    \n",
    "    #For overall ICD Statistics\n",
    "    EncounterTotal = IcuPatientIcd.drop_duplicates(['SUBJECT_ID', 'SEQ_NUM']).groupby('SUBJECT_ID')['SEQ_NUM'].count()\n",
    "    EncounterMax = max(EncounterTotal)\n",
    "    EncounterMean = np.round(np.mean(EncounterTotal), 2)\n",
    "    EncounterSTD = np.round(np.std(EncounterTotal), 2)\n",
    "    print(\"ICD max: \", EncounterMax)\n",
    "    print(\"ICD mean: \", EncounterMean)\n",
    "    print(\"ICD Standard deviation: \", EncounterSTD)\n",
    "    \n",
    "    return PatientIcdCodes, TotalIcdCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_icd_codes, total_icd_codes = Icd_Codes(Diagnosis)\n",
    "print(\"Total ICD code length: \", len(total_icd_codes))\n",
    "#assert(len(icd_codes) == 942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medication statistics\n",
    "def Medications(Prescriptions):\n",
    "    #Need advice about medications for data science uses. In nursing, each medication is necessary \n",
    "    #however the authors mension removing duplicates per encounter. Should we only be keeping one medication\n",
    "    #type per encounter? This doesnt seem like a good idea, but I am not sure if that is what the author is \n",
    "    #implying. Or, should we keep one medication type per day?\n",
    "    \n",
    "    # I am not sure where you saw that but I would assume we would keep each medication from each hospital\n",
    "    # stay but even removing the same medication from different stays is too high compared to the paper\n",
    "    \n",
    "\n",
    "    # use drug code (FORMULARY_DRUG_CD) instead of drug name (DRUG) ?\n",
    "    MedStatistics = Prescriptions[['SUBJECT_ID', 'HADM_ID', 'DRUG']]\n",
    "    IcuPatientLab = MedStatistics[MedStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    \n",
    "    NewMedStatistics = IcuPatientLab[['SUBJECT_ID', 'DRUG']]\n",
    "    IcuLabList = NewMedStatistics.drop_duplicates(['DRUG'])\n",
    "    \n",
    "    return IcuLabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = Medications(Prescriptions)\n",
    "\n",
    "print(\"Medication count: \" + str(len(meds)))\n",
    "# how many medications they ended up with in appendix 1\n",
    "#assert(len(meds) == 3202)\n",
    "# Medications\n",
    "# min per encounter: 0\n",
    "# max per encounter: 164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde54247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab component statistics. Mostly interested in abnormal labs\n",
    "\n",
    "def Laboratory(LabEvents):\n",
    "    #Only include the abnormal flags, and ignore the actual values.\n",
    "    LabStatistics = LabEvents[['SUBJECT_ID', 'ITEMID', 'FLAG']]\n",
    "    IcuPatientLab = LabStatistics[LabStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    TotalCodes = IcuPatientLab.filter([\"ITEMID\"]).drop_duplicates()\n",
    "    \n",
    "    #Create a list of all labs if needed\n",
    "    AllLabs = IcuPatientLab.groupby(\"SUBJECT_ID\")[\"ITEMID\"].apply(list)\n",
    "    \n",
    "    #Separate the lab events into all found in ICU patients. not sure if needed\n",
    "    IcuLabList = IcuPatientLab.drop_duplicates(['ITEMID'])\n",
    "    \n",
    "    #Gather only the abnormal flagged events\n",
    "    IcuPatientLab = IcuPatientLab.dropna().reset_index(drop = True)\n",
    "    IcuPatientLabList = []\n",
    "    TotalAbnormalCodes = IcuPatientLab.filter([\"ITEMID\"]).drop_duplicates()\n",
    "    AbnormalLabs = IcuPatientLab.groupby(\"SUBJECT_ID\")[\"ITEMID\"].apply(list)\n",
    "\n",
    "    #Abnormal lab statistics by patient\n",
    "    LabTotal = IcuPatientLab.drop_duplicates([\"SUBJECT_ID\", \"ITEMID\"]).groupby('SUBJECT_ID')['ITEMID'].count()\n",
    "    LabMax = max(LabTotal)\n",
    "    LabMean = np.round(np.mean(LabTotal), 2)\n",
    "    LabSTD = np.round(np.std(LabTotal), 2)\n",
    "    print(\"Abnormal Lab max: \", LabMax)\n",
    "    print(\"Abnormal Lab mean: \", LabMean)\n",
    "    print(\"Abnormal Lab Standard deviation: \", LabSTD)\n",
    "    \n",
    "    return AbnormalLabs, TotalAbnormalCodes, TotalCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_codes, total_abnormal_lab_codes, total_lab_codes = Laboratory(LabEvents)\n",
    "print(\"Abnormal codes length: \", len(total_abnormal_lab_codes))\n",
    "print(\"Total codes length: \", len(total_lab_codes))\n",
    "#assert(len(lab_codes) == 284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13517328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome label. Creates a list of 0's or 1's of length Patient_ID, where 1's indicate the patient passed away.\n",
    "\n",
    "def Mortality(Admissions, Patient_ID):\n",
    "    OutcomeFlag = Admissions[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    Outcome = OutcomeFlag[OutcomeFlag.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    Deceased = []\n",
    "\n",
    "    #Create list of deceased patients from ICU only patients\n",
    "    for i in Outcome.to_numpy():\n",
    "        if i[1] == 1:\n",
    "            Deceased.append(i[0])\n",
    "    \n",
    "    #convert to numpy and organize\n",
    "    Deceased = np.array(Deceased)\n",
    "    Deceased = pd.DataFrame(Deceased, columns = ['Deceased']).to_numpy()\n",
    "    DeceasedFlag = []\n",
    "    \n",
    "    #Create list of 0s and 1s to determine from Patient_ID which patient survived and which did not.\n",
    "    for i in Patient_ID:\n",
    "        if i in Deceased:\n",
    "            DeceasedFlag.append(1)\n",
    "        else:\n",
    "            DeceasedFlag.append(0)\n",
    "            \n",
    "    return DeceasedFlag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fecaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortality(Admissions, Patient_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to create the sequence variable by concatenating ICDs, Med codes, and lab codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35463316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we implement the custom dataset with our sequence variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we collate our data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd132707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we feed our data into the model\n",
    "#If it does not fit into the Autoencoder easily, we should implement it into Retain as our data should \n",
    "#fit into retain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model (The most important model to finish)\n",
    "\n",
    "#Source: https://github.com/subendhu19/CLOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clout outline photos to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Encoder (AE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100):\n",
    "        super(AE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "\n",
    "        return [output_full, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print 'Epoch:', epoch\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "               \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print 'Epoch loss:', np.mean(losses)\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "model = AE(10,50,175)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print 'Pickled embedding weights. Shape:', np.array(emb_weights).shape\n",
    "pickle.dump(emb_weights, open('../full data/CAE/AE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# print \"Getting embeddings\"\n",
    "# outputs = []\n",
    "\n",
    "# for i in tqdm(range(0, ICD_data.shape[0], 50)):\n",
    "# \tICDbatch, Labbatch = ICD_data[i:i+50], Lab_data[i:i+50]\n",
    "# \toutputsbatch = model.get_encodings(ICDbatch, Labbatch).data.numpy()\n",
    "# \tfor ob in outputsbatch:\n",
    "# \t\toutputs.append(ob)\n",
    "\n",
    "# pickle.dump(np.array(outputs), open('../full data/CAE/AE_Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Auto Encoder (CAE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681 284\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100, lamb=0.01):\n",
    "        super(CAE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "        input_onlyicd = torch.cat((input_icd, Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlymed = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), input_med, Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlylab = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), input_lab), 1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\n",
    "        hidden_onlyicd = F.relu(self.emb(input_onlyicd))\n",
    "        hidden_onlymed = F.relu(self.emb(input_onlymed))\t\n",
    "        hidden_onlylab = F.relu(self.emb(input_onlylab))\t\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "        output_onlyicd = F.relu(self.out(hidden_onlyicd))\n",
    "        output_onlymed = F.relu(self.out(hidden_onlymed))\n",
    "        output_onlylab = F.relu(self.out(hidden_onlylab))\t\n",
    "\n",
    "        return [output_full, output_onlyicd, output_onlymed, output_onlylab, hidden_onlyicd, hidden_onlymed, hidden_onlylab, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Med_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Med_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def correlation_coef(self, x, y):\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "\n",
    "        cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "        return cost\n",
    "\n",
    "    def joint_cumulant_by_var(self, x, y, z):\t\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "        vz = z - torch.mean(z)\n",
    "\n",
    "        cost = torch.sum(vx * vy * vz) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) * torch.sqrt(torch.sum(vz ** 2)))\n",
    "        return cost\n",
    "\n",
    "        # e_xyz = torch.mean(x * y * z)\n",
    "        # e_xy = torch.mean(x * y)\n",
    "        # e_yz = torch.mean(y * z)\n",
    "        # e_xz = torch.mean(x * z)\n",
    "        # e_x = torch.mean(x)\n",
    "        # e_y = torch.mean(y)\n",
    "        # e_z = torch.mean(z)\n",
    "\n",
    "        # kappa = e_xyz - (e_xy * e_z) - (e_xz * e_y) - (e_yz * e_x) + (2*e_x*e_y*e_z) \n",
    "\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print 'Epoch:', epoch\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss_recon = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[1], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) \\\n",
    "                        + self.reconloss(outputs[2], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[3], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                loss_cr = self.joint_cumulant_by_var(outputs[4], outputs[5], outputs[6])\n",
    "\n",
    "                loss = loss_recon - (self.lamb*loss_cr)\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print 'Epoch loss:', np.mean(losses)\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "\n",
    "model = CAE(10,50,175,0.01)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print 'Pickled embedding weights. Shape:', np.array(emb_weights).shape\n",
    "pickle.dump(emb_weights, open('../full data/CAE/CAE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# outputs = model.get_encodings(ICD_data, Med_data, Lab_data)\n",
    "# print np.array(outputs).shape\n",
    "# pickle.dump(outputs, open('../full data/CAE/Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb_icd = nn.Linear(vocabsize_icd, embsize_icd)\n",
    "        self.emb_meds = nn.Linear(vocabsize_meds, embsize_meds)\n",
    "        self.emb_labs = nn.Linear(vocabsize_labs, embsize_labs)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_icd)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        input_icd = self.emb_icd(input_icd)\n",
    "        input_med = self.emb_meds(input_med)\n",
    "        input_lab = self.emb_labs(input_lab)\n",
    "\n",
    "        inputs = F.relu(torch.cat((input_icd, input_med, input_lab, input_latent),1))\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_icd, input_med, input_lab, input_latent):\n",
    "        out, hid = self.forward(input_icd, input_med, input_lab, input_latent, None)\n",
    "        return self.sig(out[-1]).data[0]\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Coq tactic prediction')\n",
    "parser.add_argument('--emb_weights', type=str, default='../full data/CAE/CAE_embedding_weights.npy',\n",
    "                    help='location of the embedding weight file (default: ../full data/CAE/CAE_embedding_weights.npy)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_icd = 50\n",
    "embsize_meds = 75\n",
    "embsize_labs = 50\n",
    "embsize_latent = 175\n",
    "embsize = embsize_icd + embsize_labs + embsize_meds + embsize_latent\n",
    "\n",
    "input_seqs_icd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.3digitICD9.seqs')))\n",
    "input_seqs_meds = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.meds.seqs')))\n",
    "input_seqs_labs = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.abnlabs.seqs')))\n",
    "# input_seqs_latent = np.array(pickle.load(open('../full data/CAE/Embeddings.seqs')))\n",
    "latent_weights = pickle.load(open(args.emb_weights))\n",
    "\n",
    "input_seqs_fullicd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.seqs')))\n",
    "\n",
    "icditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.types', 'rb'))\n",
    "meditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.meds.types', 'rb'))\n",
    "labitems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.abnlabs.types', 'rb'))\n",
    "\n",
    "model_name = 'AE'\n",
    "if args.emb_weights == '../full data/CAE/CAE_embedding_weights.npy':\n",
    "    model_name = 'CAE'\n",
    "\n",
    "interpretation_file = open(\"RNN_CLatent_Interpretations_\" + model_name + \".txt\", 'w')\n",
    "\n",
    "# overall_risk_factor_file = open(\"risk_factors_averaged.txt\", \"w\")\n",
    "\n",
    "labnames = {}\n",
    "lab_dict_file = open('D_LABITEMS.csv', 'r')\n",
    "lab_dict_file.readline()\n",
    "for line in lab_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    labnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "lab_dict_file.close()\n",
    "\n",
    "icdnames = {}\n",
    "icd_dict_file = open('D_ICD_DIAGNOSES.csv', 'r')\n",
    "icd_dict_file.readline()\n",
    "for line in icd_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    icdnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "icd_dict_file.close()\n",
    "\n",
    "icd_scores = {}\n",
    "med_scores = {}\n",
    "lab_scores = {}\n",
    "\n",
    "icd_totals = {}\n",
    "med_totals = {}\n",
    "lab_totals = {}\n",
    "\n",
    "def get_ICD(icd):\n",
    "    ret_str = \"\"\n",
    "    icd_str = icditems.keys()[icditems.values().index(icd)]\n",
    "    actual_key = icd_str.replace(\".\", \"\")[2:]\n",
    "    if actual_key in icdnames:\n",
    "        ret_str = icdnames[actual_key]\n",
    "    else:\n",
    "        ret_str = icd_str\n",
    "    return ret_str\n",
    "\n",
    "def get_med(med):\n",
    "    ret_str = meditems.keys()[meditems.values().index(med)]\n",
    "    return ret_str\n",
    "\n",
    "def get_lab(lab):\n",
    "    ret_str = labnames[labitems.keys()[labitems.values().index(lab)]]\n",
    "    return ret_str\n",
    "\n",
    "print 'Data loaded..'\n",
    "\n",
    "labels = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.morts')))\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "print 'Data prepared..'\n",
    "\n",
    "def convert_to_one_hot(code_seqs, len):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))\n",
    "\n",
    "def get_factors(icd_seq, med_seq, lab_seq, model, actual_score, full_icd):\n",
    "    potential_test_data = []\n",
    "\n",
    "    for seq in range(len(icd_seq)):\n",
    "        for i in range(len(icd_seq[seq])):\n",
    "            potential_test_data.append((\"icd\", full_icd[seq][i], seq, icd_seq[:seq]+[icd_seq[seq][:i] + icd_seq[seq][i+1:]]+icd_seq[seq+1:], med_seq, lab_seq))\n",
    "    for seq in range(len(med_seq)):\n",
    "        for i in range(len(med_seq[seq])):\n",
    "            potential_test_data.append((\"med\", med_seq[seq][i], seq, icd_seq, med_seq[:seq]+[med_seq[seq][:i]+med_seq[seq][i+1:]]+med_seq[seq+1:], lab_seq))\n",
    "    for seq in range(len(lab_seq)):\n",
    "        for i in range(len(lab_seq[seq])):\n",
    "            potential_test_data.append((\"lab\", lab_seq[seq][i], seq, icd_seq, med_seq, lab_seq[:seq]+[lab_seq[seq][:i] + lab_seq[seq][i+1:]]+lab_seq[seq+1:]))\n",
    "\n",
    "    risk_scores = []\n",
    "\n",
    "    for pt in potential_test_data:\n",
    "        test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(pt[3], vocabsize_icd)).float())\n",
    "        test_input_med = Variable(torch.from_numpy(convert_to_one_hot(pt[4], vocabsize_meds)).float())\n",
    "        test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(pt[5], vocabsize_labs)).float())\n",
    "\n",
    "        latent_inputs_oh = np.concatenate((convert_to_one_hot(pt[3], vocabsize_icd), convert_to_one_hot(pt[4], vocabsize_meds), convert_to_one_hot(pt[5], vocabsize_labs)), 1)\n",
    "        latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "        latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "        factor_score = actual_score - model.predict(test_input_icd, test_input_med, test_input_lab, latent_inputs)\n",
    "        factor = \"\"\n",
    "        if pt[0] == 'icd':\n",
    "            icd_tag = get_ICD(pt[1])\n",
    "            factor = \"ICD-\"+icd_tag\n",
    "            if icd_tag in icd_scores:\n",
    "                icd_scores[icd_tag] += factor_score\n",
    "                icd_totals[icd_tag] += 1\n",
    "            else:\n",
    "                icd_scores[icd_tag] = factor_score\n",
    "                icd_totals[icd_tag] = 1\n",
    "        elif pt[0] == 'med':\n",
    "            med_tag = get_med(pt[1])\n",
    "            factor = \"Med-\"+med_tag\n",
    "            if med_tag in med_scores:\n",
    "                med_scores[med_tag] += factor_score\n",
    "                med_totals[med_tag] += 1\n",
    "            else:\n",
    "                med_scores[med_tag] = factor_score\n",
    "                med_totals[med_tag] = 1\n",
    "        else:\n",
    "            lab_tag = get_lab(pt[1])\n",
    "            factor = \"Lab-\"+lab_tag\n",
    "            if lab_tag in lab_scores:\n",
    "                lab_scores[lab_tag] += factor_score\n",
    "                lab_totals[lab_tag] += 1\n",
    "            else:\n",
    "                lab_scores[lab_tag] = factor_score\n",
    "                lab_totals[lab_tag] = 1\n",
    "        risk_scores.append((\"Encounter-\"+str(pt[2])+\": \"+factor, factor_score))\n",
    "\n",
    "    risk_scores.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    return risk_scores[:10]\n",
    "\n",
    "print 'Starting training..'\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print 'Run', run\n",
    "\n",
    "    perm = np.random.permutation(input_seqs_icd.shape[0])\n",
    "    rinput_seqs_icd = input_seqs_icd#[perm]\n",
    "    rinput_seqs_meds = input_seqs_meds#[perm]\n",
    "    rinput_seqs_labs = input_seqs_labs#[perm]\n",
    "    # rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = input_seqs_fullicd#[perm]\n",
    "    rlabels = labels#[perm]\n",
    "\n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    # train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    # valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    # test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = train_input_seqs_icd.shape[0]\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(size_average=False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print 'Epoch', (epoch+1)\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            # batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for i in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[i], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[i], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[i], vocabsize_labs)\n",
    "\n",
    "                icd_inputs = Variable(torch.from_numpy(icd_onehot).float())\n",
    "                med_inputs = Variable(torch.from_numpy(med_onehot).float())\n",
    "                lab_inputs = Variable(torch.from_numpy(lab_onehot).float())\n",
    "\n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "                # latent_inputs = Variable(torch.from_numpy(np.array(batch_latent[iter])).float())\n",
    "\n",
    "                targets = Variable(torch.from_numpy(batch_train_labels[i]).float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, med_inputs, lab_inputs, latent_inputs, None, force)\n",
    "\n",
    "                #print outputs[-1], targets\n",
    "                losses.append(criterion(outputs[-1], targets))\n",
    "\n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "\n",
    "        #print(epoch, epoch_loss)\n",
    "\n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "        print \"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions)\n",
    "\n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        # ICD_wise_corr = np.zeros(5)\n",
    "        # meds_wise_corr = np.zeros(5)\n",
    "        # labs_wise_corr = np.zeros(5)\n",
    "        # ICD_wise_tot = np.zeros(5)\n",
    "        # meds_wise_tot = np.zeros(5)\n",
    "        # labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "            \n",
    "            # ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            # meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            # labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print \"Test AUC_ROC: \", roc_auc_score(test_labels, predictions)\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        #fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        #pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open('roc_clout_cornn.p', 'wb'))\n",
    "        #actual_predictions = (predictions>0.5)*1\n",
    "        #print classification_report(test_labels, actual_predictions)\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print \"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs) \n",
    "\n",
    "# print \"Final testing and interpretations\"\n",
    "# predictions = np.zeros(len(test_input_seqs_icd))\n",
    "# for i in (range(len(test_input_seqs_icd))):\n",
    "# \ttest_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "# \ttest_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "# \ttest_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "# \ttest_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "# \ttest_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "# \ttest_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "# \ttest_score = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "# \tpredictions[i] = test_score\n",
    "# \ttop_risk_factors = get_factors(test_input_seqs_icd[i], test_input_seqs_meds[i], test_input_seqs_labs[i], model, test_score, test_input_seqs_fullicd[i]) \n",
    "# \tif (test_score>0.5):\n",
    "# \t\tinterpretation_file.write(\"ID: \" + str(i) + \" True label: \"+str(test_labels[i])+\"\\n\")\n",
    "# \t\tfor rf in top_risk_factors:\n",
    "# \t\t\tinterpretation_file.write(str(rf)+\"\\n\")\n",
    "# \t\tinterpretation_file.write(\"\\n\")\n",
    "\n",
    "interpretation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80438e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Graphs etc...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

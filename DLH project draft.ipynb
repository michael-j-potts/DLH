{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b66248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general instructions\n",
    "\n",
    "#2 or more patient encounters are required as the authors remove the last encounter while making predictions\n",
    "# patient mortality is the outcome label and aquired from mimic. \n",
    "#2825 out of 7537 patients died (37.9%)\n",
    "#Data set was divided into  train (5275/7537), validation (753/7537) and test (1509/7537)\n",
    "# Once the optimal parameters were selected, the model was retrained by combining train and validation (6028/7537)\n",
    "#\n",
    "#\n",
    "#********************** stars mean the data is not correct but the implemention functions.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b683be-1b9a-4270-952b-a8e5a9813fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from __future__ import division\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c5b59-3e7a-4490-aa61-708d22d07470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed - only really necessary to compare results with each other, but can remove from the submission\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = \"mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "\n",
    "# define data - separated to show which modules load (some take much longer than others)\n",
    "#The commented out sections take a long time to load, and weren't immidiately useful at this stage\n",
    "#Can uncommment when relevant\n",
    "\n",
    "#TODO\n",
    "# - reorganize into a definition() after  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatientsInfo = pd.read_csv(DATA_PATH + '/PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef884c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnosis = pd.read_csv(DATA_PATH + '/DIAGNOSES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Procedures = pd.read_csv(DATA_PATH + '/D_ICD_PROCEDURES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba977e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Prescriptions = pd.read_csv(DATA_PATH + '/PRESCRIPTIONS.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef15cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabEvents = pd.read_csv(DATA_PATH + '/LABEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabLabels = pd.read_csv(DATA_PATH + '/D_LABITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Admissions = pd.read_csv(DATA_PATH + '/ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcedureLabels = pd.read_csv(DATA_PATH + '/D_ITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea694043",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcuStays = pd.read_csv(DATA_PATH + '/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChartEvents = pd.read_csv(DATA_PATH + '/CHARTEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient inclusion - Only ICU patients with 2 or more visits are included. Patient IDs gathered\n",
    "#May be incorrect as the paper specifies only 7,537 patients in their list.\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#********************************************\n",
    "#Generate an ignore list for later ease of use.\n",
    "#Manually verified that all the included patients have 2 or more subject ids. Refine by HADM_ID? ICUSTAY_ID?\n",
    "\n",
    "def Patient_Inclusion(IcuStays):\n",
    "    # Note that an encounter represents a single visit or \n",
    "    # admission of the patient to the hospital Intensive Care Unit (ICU)\n",
    "    PatientVisits = IcuStays[['SUBJECT_ID', 'ICUSTAY_ID', 'HADM_ID']]\n",
    "    print(\"Original data\")\n",
    "    print(\"ICU visits shape: \", PatientVisits.shape)\n",
    "    print(\"Number of patients: \", len(PatientVisits.SUBJECT_ID.unique()))\n",
    "    print()\n",
    "    \n",
    "    #HADM_ID_LOOKUP = IcuStays[['ICUSTAY_ID', 'HADM_ID']]\n",
    "    #print(\"ICU visits shape: \", HADM_ID_LOOKUP.shape)\n",
    "    #print(\"Number of icu stays: \", len(PatientVisits.ICUSTAY_ID.unique()))\n",
    "    \n",
    "    PatientVisits['VISIT_COUNT'] = PatientVisits.sort_values(['SUBJECT_ID', 'ICUSTAY_ID']).groupby(['SUBJECT_ID'])['ICUSTAY_ID'].transform('count')\n",
    "    print(\"After counting visits per patient\")\n",
    "    print(\"ICU visits shape: \", PatientVisits.shape)\n",
    "    print(\"Number of patients: \", len(PatientVisits.SUBJECT_ID.unique()))\n",
    "    print()\n",
    "    \n",
    "    PatientVisits = PatientVisits.query('VISIT_COUNT > 1')\n",
    "    print(\"After filtering for VISIT_COUNT > 1\")\n",
    "    print(\"ICU visits shape: \", PatientVisits.shape)\n",
    "    print(\"Number of patients: \", len(PatientVisits.SUBJECT_ID.unique()))\n",
    "    print()\n",
    "    \n",
    "    PatientVisits = PatientVisits[['SUBJECT_ID', 'ICUSTAY_ID', 'HADM_ID']]\n",
    "    PatientSeries = PatientVisits.groupby(\"SUBJECT_ID\")[\"ICUSTAY_ID\"].apply(list)\n",
    "    #This variable contains Subject_ID, Icustay_id and Hadm_id\n",
    "    HadmId = PatientVisits\n",
    "    \n",
    "    Patients = PatientSeries.index.tolist()\n",
    "    Visits = PatientSeries.tolist()\n",
    "\n",
    "    return Patients, Visits, HadmId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b177839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be added to main at the end\n",
    "Patients, Visits, HadmId = Patient_Inclusion(IcuStays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774e63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Statistics\n",
    "maxcount = len(Visits[0])\n",
    "mincount = len(Visits[0])\n",
    "for i in Visits:\n",
    "    if len(i) > maxcount:\n",
    "        maxcount = len(i)\n",
    "    if len(i) < mincount:\n",
    "        mincount = mincount\n",
    "\n",
    "print(\"Number of Patients: \", len(Patients))\n",
    "print(\"Len of Visits: \", len(Visits))        \n",
    "print(\"Max visits for a patient: \", maxcount)\n",
    "print(\"Min visits for a patient: \", mincount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021b663-956f-47c5-87dc-ec28f2029ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data statistics - Ethnicity and visit statistics\n",
    "\n",
    "def Ethnicity_Statistics(Admissions):\n",
    "    white = 0\n",
    "    black = 0\n",
    "    hispanic = 0\n",
    "    asian = 0\n",
    "    other = 0\n",
    "    \n",
    "    #separate out ethnicity information\n",
    "    EthStatistics = Admissions[['SUBJECT_ID', 'ETHNICITY']]\n",
    "    IcuPatientEth = EthStatistics[EthStatistics.SUBJECT_ID.isin(Patients)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "\n",
    "    #Not yet working.... General idea.\n",
    "    for i in IcuPatientEth.ETHNICITY:\n",
    "        if \"WHITE\" in i:\n",
    "            white += 1\n",
    "        if \"BLACK\" in i:\n",
    "            black += 1\n",
    "        if \"HISPANIC\" in i:\n",
    "            hispanic += 1\n",
    "        if \"ASIAN\"  in i:\n",
    "            asian += 1\n",
    "    \n",
    "        #I tried combining it, but the count was always more wrong...\n",
    "        if \"OTHER\" in i:\n",
    "            other += 1\n",
    "        if \"DECLINED\" in i:\n",
    "            other += 1\n",
    "        if \"NATIVE\" in i:\n",
    "            other += 1\n",
    "        if \"MIDDLE EASTERN\" in i:\n",
    "            other += 1\n",
    "        if \"MULTI\" in i:\n",
    "            other += 1\n",
    "        if \"UNABLE\" in i:\n",
    "            other += 1\n",
    "        if \"UNKNOWN\" in i:\n",
    "            other += 1\n",
    "        \n",
    "\n",
    "    print(np.round(white / len(IcuPatientEth), 3), \"percent white\")\n",
    "    print(np.round(black / len(IcuPatientEth), 3), \"percent black\")\n",
    "    print(np.round(hispanic / len(IcuPatientEth), 3), \"percent hispanic\")\n",
    "    print(np.round(asian / len(IcuPatientEth), 3), \"percent asian\")\n",
    "    print(np.round(other / len(IcuPatientEth), 3), \"percent other\")\n",
    "\n",
    "    print((np.round((white + black + hispanic + asian + other) / len(IcuPatientEth), 2)), \"percent total\")\n",
    "    \n",
    "    #*****************************************************\n",
    "    #6 too many values. Need to sort that out.... maybe? it doesnt actually really affect anything...\n",
    "    print(len(IcuPatientEth))\n",
    "    print(white + black + asian + hispanic + other)\n",
    "    return white, black, hispanic, asian, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be added to main at the end\n",
    "white, black, hispanic, asian, other = Ethnicity_Statistics(Admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50730d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age statistics - Date of birth DOB is in patients, and admissions time is in admissions.\n",
    "# need to correlate both to find patients true ages\n",
    "#Sorting to ensure that each patient id is matched in case of misordering\n",
    "def Age_statistics(PatientsInfo):\n",
    "\n",
    "    #separate out date of birth information and sort\n",
    "    AgeStatistics = PatientsInfo[['SUBJECT_ID', 'DOB']]\n",
    "    IcuPatientDob = AgeStatistics[AgeStatistics.SUBJECT_ID.isin(Patients)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientDob = IcuPatientDob.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #separate out admission date and sort\n",
    "    AdmStatistics = Admissions[['SUBJECT_ID', 'ADMITTIME']]\n",
    "    IcuPatientAdm = AdmStatistics[AdmStatistics.SUBJECT_ID.isin(Patients)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientAdm = IcuPatientAdm.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #minus dates to find ages\n",
    "    #********************************************************\n",
    "    # not done yet. need to convert at least part of the date to an int for each dataframe\n",
    "    #Year and month should be sufficient, but year month date would increase accuracy. remove time.\n",
    "    \n",
    "    \n",
    "Age_statistics(PatientsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b781bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex statistics\n",
    "def Gender_Statistics(PatientsInfo):\n",
    "    male = 0\n",
    "    female = 0\n",
    "    \n",
    "    SexStatistics = PatientsInfo[['SUBJECT_ID', 'GENDER']]\n",
    "    IcuPatientSex = SexStatistics[SexStatistics.SUBJECT_ID.isin(Patients)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    \n",
    "    for i in IcuPatientSex.GENDER:\n",
    "        if i == \"M\":\n",
    "            male += 1\n",
    "        if i == \"F\":\n",
    "            female += 1\n",
    "            \n",
    "    print(\"Male patients: \", male)\n",
    "    print(\"Female patients: \", female)\n",
    "    \n",
    "    print(np.round(male / len(IcuPatientSex), 2), \" percent male\")\n",
    "    print(np.round(female / len(IcuPatientSex), 2), \"percent female\")\n",
    "    \n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "male, female = Gender_Statistics(PatientsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICD Code statistics\n",
    "def Icd_Codes(Diagnosis):\n",
    "    \n",
    "    #Isolate just the ICD codes and sequence of ICD codes\n",
    "    IcdStatistics = Diagnosis[['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'SEQ_NUM']]\n",
    "    IcdStatistics.SEQ_NUM = IcdStatistics.SEQ_NUM.dropna()\n",
    "    IcuPatientIcd = IcdStatistics[IcdStatistics.SUBJECT_ID.isin(Patients)].reset_index(drop = True)\n",
    "    \n",
    "    #To calculate the total unique ICD codes present\n",
    "    TotalIcdCode = IcuPatientIcd.filter([\"ICD9_CODE\"]).drop_duplicates()\n",
    "    \n",
    "    #For ICD code groupings by patient id\n",
    "    PatientIcdCodes = IcuPatientIcd.groupby([\"SUBJECT_ID\", \"HADM_ID\"])[\"ICD9_CODE\"].apply(list)\n",
    "    \n",
    "    #For overall ICD Statistics\n",
    "    EncounterTotal = IcuPatientIcd.drop_duplicates(['SUBJECT_ID', 'SEQ_NUM']).groupby('SUBJECT_ID')['SEQ_NUM'].count()\n",
    "    EncounterMax = max(EncounterTotal)\n",
    "    EncounterMean = np.round(np.mean(EncounterTotal), 2)\n",
    "    EncounterSTD = np.round(np.std(EncounterTotal), 2)\n",
    "    print(\"ICD max: \", EncounterMax)\n",
    "    print(\"ICD mean: \", EncounterMean)\n",
    "    print(\"ICD Standard deviation: \", EncounterSTD)\n",
    "    \n",
    "    return PatientIcdCodes, TotalIcdCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatientIcdCodes, TotalIcdCodes = Icd_Codes(Diagnosis)\n",
    "print(\"Total ICD codes: \", len(TotalIcdCodes))\n",
    "#assert(len(icd_codes) == 942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medication statistics\n",
    "def Medications(Prescriptions):\n",
    "    #Need advice about medications for data science uses. In nursing, each medication is necessary \n",
    "    #however the authors mension removing duplicates per encounter. Should we only be keeping one medication\n",
    "    #type per encounter? This doesnt seem like a good idea, but I am not sure if that is what the author is \n",
    "    #implying. Or, should we keep one medication type per day?\n",
    "    \n",
    "    # I am not sure where you saw that but I would assume we would keep each medication from each hospital\n",
    "    # stay but even removing the same medication from different stays is too high compared to the paper\n",
    "    \n",
    "\n",
    "    # use drug code (FORMULARY_DRUG_CD) instead of drug name (DRUG) ?\n",
    "    \n",
    "    MedStatistics = Prescriptions[['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'DRUG']]\n",
    "    MedStatistics = MedStatistics[MedStatistics.SUBJECT_ID.isin(Patients)].reset_index(drop = True)\n",
    "    \n",
    "    Num_Reports = MedStatistics[['ROW_ID']]\n",
    "    print(\"Number of medication reports: \", len(Num_Reports))\n",
    "    \n",
    "    Num_Patients = MedStatistics[['SUBJECT_ID']]\n",
    "    Num_Patients = Num_Patients.drop_duplicates(['SUBJECT_ID'])\n",
    "    print(\"Number of patients with meds: \", len(Num_Patients))\n",
    "    \n",
    "    # get list of just possible medications\n",
    "    All_Meds = MedStatistics[['DRUG']]\n",
    "    All_Meds = All_Meds.drop_duplicates(['DRUG'])\n",
    "    print(\"Number of meds: \", len(All_Meds))\n",
    "    \n",
    "    # get drugs per patient, drop duplicate drugs for the same patient\n",
    "    NewMedStatistics = MedStatistics[['SUBJECT_ID', 'HADM_ID', 'DRUG']]\n",
    "    MedsPerPatient = NewMedStatistics.drop_duplicates()\n",
    "    \n",
    "    MedsPerPatient = MedsPerPatient.groupby([\"SUBJECT_ID\", \"HADM_ID\"])[\"DRUG\"].apply(list)\n",
    "    print(MedsPerPatient.head(10))\n",
    "    \n",
    "    #print(MedsPerPatient)\n",
    "    \n",
    "    return MedsPerPatient, All_Meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Patient count: \", len(Patients))\n",
    "MedsPerPatient, All_Meds = Medications(Prescriptions)\n",
    "\n",
    "# Without SUBJECT_ID filter\n",
    "# Number of medication reports:  1048575\n",
    "# Number of patients with meds:  10333\n",
    "# Number of meds:  2726\n",
    "# Medication count: 10333\n",
    "\n",
    "# With SUBJECT_ID filter\n",
    "# Number of medication reports:  539950\n",
    "# Number of patients with meds:  2397\n",
    "# Number of meds:  2213\n",
    "# Medication count: 2397\n",
    "\n",
    "print(\"Medication count: \", len(MedsPerPatient))\n",
    "# how many medications they ended up with in appendix 1\n",
    "#assert(len(meds) == 3202)\n",
    "# Medications\n",
    "# min per encounter: 0\n",
    "# max per encounter: 164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde54247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab component statistics. Mostly interested in abnormal labs\n",
    "\n",
    "def Laboratory(LabEvents):\n",
    "    #Only include the abnormal flags, and ignore the actual values.\n",
    "    LabStatistics = LabEvents[['SUBJECT_ID', 'HADM_ID', 'ITEMID', 'FLAG']]\n",
    "    IcuPatientLab = LabStatistics[LabStatistics.SUBJECT_ID.isin(Patients)].reset_index(drop = True)\n",
    "    TotalCodes = IcuPatientLab.filter([\"ITEMID\"]).drop_duplicates()\n",
    "    \n",
    "    #Create a list of all labs if needed\n",
    "    AllLabs = IcuPatientLab.groupby(\"SUBJECT_ID\")[\"ITEMID\"].apply(list)\n",
    "    \n",
    "    #Separate the lab events into all found in ICU patients. not sure if needed\n",
    "    IcuLabList = IcuPatientLab.drop_duplicates(['ITEMID'])\n",
    "    \n",
    "    #Gather only the abnormal flagged events\n",
    "    IcuPatientLab = IcuPatientLab.dropna().reset_index(drop = True)\n",
    "    IcuPatientLabList = []\n",
    "    TotalAbnormalCodes = IcuPatientLab.filter([\"ITEMID\"]).drop_duplicates()\n",
    "    IcuPatientLab.HADM_ID = IcuPatientLab.HADM_ID.astype('int')\n",
    "    AbnormalLabs = IcuPatientLab.groupby([\"SUBJECT_ID\", \"HADM_ID\"])[\"ITEMID\"].apply(list)\n",
    "\n",
    "    #Abnormal lab statistics by patient\n",
    "    LabTotal = IcuPatientLab.drop_duplicates([\"SUBJECT_ID\", \"ITEMID\"]).groupby('SUBJECT_ID')['ITEMID'].count()\n",
    "    LabMax = max(LabTotal)\n",
    "    LabMean = np.round(np.mean(LabTotal), 2)\n",
    "    LabSTD = np.round(np.std(LabTotal), 2)\n",
    "    print(\"Abnormal Lab max: \", LabMax)\n",
    "    print(\"Abnormal Lab mean: \", LabMean)\n",
    "    print(\"Abnormal Lab Standard deviation: \", LabSTD)\n",
    "    \n",
    "    return AbnormalLabs, TotalAbnormalCodes, TotalCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AbnormalLabs, TotalAbnormalCodes, TotalCodes = Laboratory(LabEvents)\n",
    "print(\"Total abnormal lab codes: \", len(TotalAbnormalCodes))\n",
    "print(\"Total lab codes: \", len(TotalCodes))\n",
    "#assert(len(TotalAbnormalCodes == 284))\n",
    "#assert(len(TotalCodes) == 681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13517328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome label. Creates a list of 0's or 1's of length Patients, where 1's indicate the patient passed away.\n",
    "\n",
    "def Mortality(Admissions, Patients):\n",
    "    OutcomeFlag = Admissions[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    Outcome = OutcomeFlag[OutcomeFlag.SUBJECT_ID.isin(Patients)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    Deceased = []\n",
    "\n",
    "    #Create list of deceased patients from ICU only patients\n",
    "    for i in Outcome.to_numpy():\n",
    "        if i[1] == 1:\n",
    "            Deceased.append(i[0])\n",
    "    \n",
    "    #convert to numpy and organize\n",
    "    Deceased = np.array(Deceased)\n",
    "    Deceased = pd.DataFrame(Deceased, columns = ['Deceased']).to_numpy()\n",
    "    DeceasedFlag = []\n",
    "    \n",
    "    #Create list of 0s and 1s to determine from Patients which patient survived and which did not.\n",
    "    for i in Patients:\n",
    "        if i in Deceased:\n",
    "            DeceasedFlag.append(1)\n",
    "        else:\n",
    "            DeceasedFlag.append(0)\n",
    "            \n",
    "    return DeceasedFlag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fecaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = Mortality(Admissions, Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(PatientIcdCodes, MedsPerPatient, AbnormalLabs):\n",
    "    #Reform into dataframes\n",
    "    IcdCodes = pd.DataFrame(PatientIcdCodes)\n",
    "    MedCodes = pd.DataFrame(MedsPerPatient)\n",
    "    LabCodes = pd.DataFrame(AbnormalLabs)\n",
    "\n",
    "    #Merge all together keeping both patient id and visits separate\n",
    "    IcdMeds = pd.merge(IcdCodes, MedCodes, how = \"left\", on = [\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    IcdMedsLabs = pd.merge(IcdMeds, LabCodes, how = \"left\", on = [\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    IcdMedsLabs = pd.merge(IcdMeds, LabCodes, how = \"left\", on = [\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    IcdMedsLabs = IcdMedsLabs.fillna(\"\").apply(list)\n",
    "    \n",
    "    # combine codes now to make the list conversion easier\n",
    "    #IcdMedsLabs['CODES']= IcdMedsLabs.values.tolist()\n",
    "    IcdMedsLabs = IcdMedsLabs.reset_index()\n",
    "    display(IcdMedsLabs.head(5))\n",
    "    ICD_CODES = IcdMedsLabs[[\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\"]]\n",
    "    MED_CODES = IcdMedsLabs[[\"SUBJECT_ID\", \"HADM_ID\", \"DRUG\"]]\n",
    "    LAB_CODES = IcdMedsLabs[[\"SUBJECT_ID\", \"HADM_ID\", \"ITEMID\"]]\n",
    "    \n",
    "    IcdList = ICD_CODES.groupby(\"SUBJECT_ID\")[\"ICD9_CODE\"].apply(list)\n",
    "    IcdList = IcdList.tolist()\n",
    "    \n",
    "    MedList = MED_CODES.groupby(\"SUBJECT_ID\")[\"DRUG\"].apply(list)\n",
    "    MedList = MedList.tolist()\n",
    "    \n",
    "    LabList = LAB_CODES.groupby(\"SUBJECT_ID\")[\"ITEMID\"].apply(list)\n",
    "    LabList = LabList.tolist()\n",
    "    \n",
    "    return IcdList, MedList, LabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73663953",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcdList, MedList, LabList = PrepareData(PatientIcdCodes, MedsPerPatient, AbnormalLabs)\n",
    "print(IcdList[0][0])\n",
    "print(MedList[0][0])\n",
    "print(LabList[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35463316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we implement the custom dataset with our sequence variable\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, IcdList, MedList, LabList, outcome):\n",
    "        self.Icd = IcdList\n",
    "        self.Meds = MedList\n",
    "        self.Labs = LabList\n",
    "        self.y = outcome\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(outcome)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.Icd[index], self.Meds[index], self.Labs[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b759943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run dataset\n",
    "dataset = CustomDataset(IcdList, MedList, LabList, outcome)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This collate function is currently experimental. It may or may not be the correct way to implement \n",
    "#a collate function for multiple datasets of information.\n",
    "#The alternate method would be to use the course HW collate function, feed ICD, meds, and labs in\n",
    "#Individually and then concatenate them after. Unsure currently.\n",
    "\n",
    "def custom_collate_fn(data):\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    #num_meds = [len(visit) for ]\n",
    "    #num_labs = ...\n",
    "    \n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_icd = max(num_codes)\n",
    "    max_num_meds = max(num_meds)\n",
    "    max_num_labs = max(num_labs)\n",
    "    \n",
    "    icd_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    med_x = torch.zeros((num_patients, max_num_visits, max_num_meds), dtype=torch.long)\n",
    "    lab_x = torch.zeros((num_patients, max_num_visits, max_num_labs), dtype=torch.long)\n",
    "    \n",
    "    reverse_icdx = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    reverse_medx = torch.zeros((num_patients, max_num_visits, max_num_meds), dtype=torch.long)\n",
    "    reverse_labx = torch.zeros((num_patients, max_num_visits, max_num_labs), dtype=torch.long)\n",
    "    \n",
    "    icd_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    med_masks = torch.zeros((num_patients, max_num_visits, max_num_meds), dtype=torch.bool)\n",
    "    lab_masks = torch.zeros((num_patients, max_num_visits, max_num_labs), dtype=torch.bool)\n",
    "    \n",
    "    reverse_icd_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    reverse_med_masks = torch.zeros((num_patients, max_num_visits, max_num_meds), dtype=torch.bool)\n",
    "    reverse_lab_masks = torch.zeros((num_patients, max_num_visits, max_num_labs), dtype=torch.bool)\n",
    "    \n",
    "    \n",
    "    #need to alter this completely\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            tot_p = len(patient)\n",
    "            tot_v = len(visit)\n",
    "            x[i_patient, j_visit, :tot_v] = torch.tensor(visit, dtype = torch.long)\n",
    "            rev_x[i_patient, tot_p-j_visit-1, :tot_v] = torch.tensor(visit, dtype = torch.long)\n",
    "            masks[i_patient, j_visit, :tot_v].fill_(1)\n",
    "            rev_masks[i_patient, tot_p-j_visit-1, :tot_v].fill_(1)\n",
    "            \n",
    "            \n",
    "    \n",
    "    return icd_x, med_x, lab_x, reverse_icdx, reverse_medx, reverse_labx, icd_masks, med_masks, lab_masks, reverse_icd_masks, reverse_med_masks, reverse_lab_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain customdataset\n",
    "\n",
    "#Need to rename the other class to avoid errors.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ICD_Codes, outcome):\n",
    "        self.x = ICD_Codes\n",
    "        self.y = outcome\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(IcdList, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used for our ICD only Retain which is described below\n",
    "\n",
    "def retain_collate_fn(data):\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            tot_p = len(patient)\n",
    "            tot_v = len(visit)\n",
    "            x[i_patient, j_visit, :tot_v] = torch.tensor(visit, dtype = torch.long)\n",
    "            rev_x[i_patient, tot_p-j_visit-1, :tot_v] = torch.tensor(visit, dtype = torch.long)\n",
    "            masks[i_patient, j_visit, :tot_v].fill_(1)\n",
    "            rev_masks[i_patient, tot_p-j_visit-1, :tot_v].fill_(1)\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3032dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split our data into train, validate and test as per the papers values\n",
    "split = int(len(dataset) * 0.7)\n",
    "split2 = int(len(dataset) * 0.1)\n",
    "split3 = int(len(dataset) * 0.2002)\n",
    "lengths = [split, split2, split3]\n",
    "print(len(dataset))\n",
    "print(sum(lengths))\n",
    "print(lengths)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of training dataset: \", len(train_dataset))\n",
    "print(\"Length of validation dataset: \", len(val_dataset))\n",
    "print(\"Length of test dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e889abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loader to create train validation and test splits\n",
    "def load_data(train_dataset, val_dataset, test_dataset, retain_collate_fn):\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, collate_fn = retain_collate_fn)\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, collate_fn = retain_collate_fn)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, collate_fn = retain_collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d054850",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, retain_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain model - ICD codes only\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        x = self.a_att(g)\n",
    "        softmax = torch.nn.Softmax(dim = 1)\n",
    "        x = softmax(x)\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7807f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        x = self.b_att(h)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    rev_masks = torch.unsqueeze(torch.max(rev_masks, dim = 2)[0], dim = -1)\n",
    "    c = alpha * beta * (rev_v * rev_masks)\n",
    "    c = torch.sum(c, dim = 1)\n",
    "    return c    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346be5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`;\n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        # Define the beta-attention using `BetaAttention()`;\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        # Define the linear layers using `nn.Linear()`;\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        # 1. Pass the reversed sequence through the embedding layer;\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        # 5. Sum the attention up using `attention_sum()`;\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        # 6. Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze()\n",
    "\n",
    "# load the model here\n",
    "retain = RETAIN(num_codes = len(TotalIcdCodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_logit = model(x, masks, rev_x, rev_masks)\n",
    "        y_hat = y_logit > 0.5\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11707351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "retain = RETAIN(num_codes = len(TotalIcdCodes))\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 10\n",
    "train(retain, train_loader, val_loader, n_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd132707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we feed our data into the model\n",
    "#If it does not fit into the Autoencoder easily, we should implement it into Retain as our data should \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7ef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model (The most important model to finish)\n",
    "\n",
    "#Source: https://github.com/subendhu19/CLOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clout outline photos to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Encoder (AE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100):\n",
    "        super(AE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "\n",
    "        return [output_full, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch:', epoch)\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "               \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print('Epoch loss:', np.mean(losses))\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "model = AE(10,50,175)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print('Pickled embedding weights. Shape:', np.array(emb_weights).shape)\n",
    "pickle.dump(emb_weights, open('../full data/CAE/AE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# print \"Getting embeddings\"\n",
    "# outputs = []\n",
    "\n",
    "# for i in tqdm(range(0, ICD_data.shape[0], 50)):\n",
    "# \tICDbatch, Labbatch = ICD_data[i:i+50], Lab_data[i:i+50]\n",
    "# \toutputsbatch = model.get_encodings(ICDbatch, Labbatch).data.numpy()\n",
    "# \tfor ob in outputsbatch:\n",
    "# \t\toutputs.append(ob)\n",
    "\n",
    "# pickle.dump(np.array(outputs), open('../full data/CAE/AE_Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Auto Encoder (CAE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681 284\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100, lamb=0.01):\n",
    "        super(CAE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "        input_onlyicd = torch.cat((input_icd, Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlymed = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), input_med, Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlylab = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), input_lab), 1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\n",
    "        hidden_onlyicd = F.relu(self.emb(input_onlyicd))\n",
    "        hidden_onlymed = F.relu(self.emb(input_onlymed))\t\n",
    "        hidden_onlylab = F.relu(self.emb(input_onlylab))\t\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "        output_onlyicd = F.relu(self.out(hidden_onlyicd))\n",
    "        output_onlymed = F.relu(self.out(hidden_onlymed))\n",
    "        output_onlylab = F.relu(self.out(hidden_onlylab))\t\n",
    "\n",
    "        return [output_full, output_onlyicd, output_onlymed, output_onlylab, hidden_onlyicd, hidden_onlymed, hidden_onlylab, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Med_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Med_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def correlation_coef(self, x, y):\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "\n",
    "        cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "        return cost\n",
    "\n",
    "    def joint_cumulant_by_var(self, x, y, z):\t\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "        vz = z - torch.mean(z)\n",
    "\n",
    "        cost = torch.sum(vx * vy * vz) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) * torch.sqrt(torch.sum(vz ** 2)))\n",
    "        return cost\n",
    "\n",
    "        # e_xyz = torch.mean(x * y * z)\n",
    "        # e_xy = torch.mean(x * y)\n",
    "        # e_yz = torch.mean(y * z)\n",
    "        # e_xz = torch.mean(x * z)\n",
    "        # e_x = torch.mean(x)\n",
    "        # e_y = torch.mean(y)\n",
    "        # e_z = torch.mean(z)\n",
    "\n",
    "        # kappa = e_xyz - (e_xy * e_z) - (e_xz * e_y) - (e_yz * e_x) + (2*e_x*e_y*e_z) \n",
    "\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print 'Epoch:', epoch\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss_recon = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[1], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) \\\n",
    "                        + self.reconloss(outputs[2], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[3], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                loss_cr = self.joint_cumulant_by_var(outputs[4], outputs[5], outputs[6])\n",
    "\n",
    "                loss = loss_recon - (self.lamb*loss_cr)\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print 'Epoch loss:', np.mean(losses)\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "\n",
    "model = CAE(10,50,175,0.01)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print 'Pickled embedding weights. Shape:', np.array(emb_weights).shape\n",
    "pickle.dump(emb_weights, open('../full data/CAE/CAE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# outputs = model.get_encodings(ICD_data, Med_data, Lab_data)\n",
    "# print np.array(outputs).shape\n",
    "# pickle.dump(outputs, open('../full data/CAE/Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb_icd = nn.Linear(vocabsize_icd, embsize_icd)\n",
    "        self.emb_meds = nn.Linear(vocabsize_meds, embsize_meds)\n",
    "        self.emb_labs = nn.Linear(vocabsize_labs, embsize_labs)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_icd)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        input_icd = self.emb_icd(input_icd)\n",
    "        input_med = self.emb_meds(input_med)\n",
    "        input_lab = self.emb_labs(input_lab)\n",
    "\n",
    "        inputs = F.relu(torch.cat((input_icd, input_med, input_lab, input_latent),1))\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_icd, input_med, input_lab, input_latent):\n",
    "        out, hid = self.forward(input_icd, input_med, input_lab, input_latent, None)\n",
    "        return self.sig(out[-1]).data[0]\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Coq tactic prediction')\n",
    "parser.add_argument('--emb_weights', type=str, default='../full data/CAE/CAE_embedding_weights.npy',\n",
    "                    help='location of the embedding weight file (default: ../full data/CAE/CAE_embedding_weights.npy)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_icd = 50\n",
    "embsize_meds = 75\n",
    "embsize_labs = 50\n",
    "embsize_latent = 175\n",
    "embsize = embsize_icd + embsize_labs + embsize_meds + embsize_latent\n",
    "\n",
    "input_seqs_icd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.3digitICD9.seqs')))\n",
    "input_seqs_meds = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.meds.seqs')))\n",
    "input_seqs_labs = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.abnlabs.seqs')))\n",
    "# input_seqs_latent = np.array(pickle.load(open('../full data/CAE/Embeddings.seqs')))\n",
    "latent_weights = pickle.load(open(args.emb_weights))\n",
    "\n",
    "input_seqs_fullicd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.seqs')))\n",
    "\n",
    "icditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.types', 'rb'))\n",
    "meditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.meds.types', 'rb'))\n",
    "labitems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.abnlabs.types', 'rb'))\n",
    "\n",
    "model_name = 'AE'\n",
    "if args.emb_weights == '../full data/CAE/CAE_embedding_weights.npy':\n",
    "    model_name = 'CAE'\n",
    "\n",
    "interpretation_file = open(\"RNN_CLatent_Interpretations_\" + model_name + \".txt\", 'w')\n",
    "\n",
    "# overall_risk_factor_file = open(\"risk_factors_averaged.txt\", \"w\")\n",
    "\n",
    "labnames = {}\n",
    "lab_dict_file = open('D_LABITEMS.csv', 'r')\n",
    "lab_dict_file.readline()\n",
    "for line in lab_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    labnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "lab_dict_file.close()\n",
    "\n",
    "icdnames = {}\n",
    "icd_dict_file = open('D_ICD_DIAGNOSES.csv', 'r')\n",
    "icd_dict_file.readline()\n",
    "for line in icd_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    icdnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "icd_dict_file.close()\n",
    "\n",
    "icd_scores = {}\n",
    "med_scores = {}\n",
    "lab_scores = {}\n",
    "\n",
    "icd_totals = {}\n",
    "med_totals = {}\n",
    "lab_totals = {}\n",
    "\n",
    "def get_ICD(icd):\n",
    "    ret_str = \"\"\n",
    "    icd_str = icditems.keys()[icditems.values().index(icd)]\n",
    "    actual_key = icd_str.replace(\".\", \"\")[2:]\n",
    "    if actual_key in icdnames:\n",
    "        ret_str = icdnames[actual_key]\n",
    "    else:\n",
    "        ret_str = icd_str\n",
    "    return ret_str\n",
    "\n",
    "def get_med(med):\n",
    "    ret_str = meditems.keys()[meditems.values().index(med)]\n",
    "    return ret_str\n",
    "\n",
    "def get_lab(lab):\n",
    "    ret_str = labnames[labitems.keys()[labitems.values().index(lab)]]\n",
    "    return ret_str\n",
    "\n",
    "print 'Data loaded..'\n",
    "\n",
    "labels = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.morts')))\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "print 'Data prepared..'\n",
    "\n",
    "def convert_to_one_hot(code_seqs, len):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))\n",
    "\n",
    "def get_factors(icd_seq, med_seq, lab_seq, model, actual_score, full_icd):\n",
    "    potential_test_data = []\n",
    "\n",
    "    for seq in range(len(icd_seq)):\n",
    "        for i in range(len(icd_seq[seq])):\n",
    "            potential_test_data.append((\"icd\", full_icd[seq][i], seq, icd_seq[:seq]+[icd_seq[seq][:i] + icd_seq[seq][i+1:]]+icd_seq[seq+1:], med_seq, lab_seq))\n",
    "    for seq in range(len(med_seq)):\n",
    "        for i in range(len(med_seq[seq])):\n",
    "            potential_test_data.append((\"med\", med_seq[seq][i], seq, icd_seq, med_seq[:seq]+[med_seq[seq][:i]+med_seq[seq][i+1:]]+med_seq[seq+1:], lab_seq))\n",
    "    for seq in range(len(lab_seq)):\n",
    "        for i in range(len(lab_seq[seq])):\n",
    "            potential_test_data.append((\"lab\", lab_seq[seq][i], seq, icd_seq, med_seq, lab_seq[:seq]+[lab_seq[seq][:i] + lab_seq[seq][i+1:]]+lab_seq[seq+1:]))\n",
    "\n",
    "    risk_scores = []\n",
    "\n",
    "    for pt in potential_test_data:\n",
    "        test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(pt[3], vocabsize_icd)).float())\n",
    "        test_input_med = Variable(torch.from_numpy(convert_to_one_hot(pt[4], vocabsize_meds)).float())\n",
    "        test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(pt[5], vocabsize_labs)).float())\n",
    "\n",
    "        latent_inputs_oh = np.concatenate((convert_to_one_hot(pt[3], vocabsize_icd), convert_to_one_hot(pt[4], vocabsize_meds), convert_to_one_hot(pt[5], vocabsize_labs)), 1)\n",
    "        latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "        latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "        factor_score = actual_score - model.predict(test_input_icd, test_input_med, test_input_lab, latent_inputs)\n",
    "        factor = \"\"\n",
    "        if pt[0] == 'icd':\n",
    "            icd_tag = get_ICD(pt[1])\n",
    "            factor = \"ICD-\"+icd_tag\n",
    "            if icd_tag in icd_scores:\n",
    "                icd_scores[icd_tag] += factor_score\n",
    "                icd_totals[icd_tag] += 1\n",
    "            else:\n",
    "                icd_scores[icd_tag] = factor_score\n",
    "                icd_totals[icd_tag] = 1\n",
    "        elif pt[0] == 'med':\n",
    "            med_tag = get_med(pt[1])\n",
    "            factor = \"Med-\"+med_tag\n",
    "            if med_tag in med_scores:\n",
    "                med_scores[med_tag] += factor_score\n",
    "                med_totals[med_tag] += 1\n",
    "            else:\n",
    "                med_scores[med_tag] = factor_score\n",
    "                med_totals[med_tag] = 1\n",
    "        else:\n",
    "            lab_tag = get_lab(pt[1])\n",
    "            factor = \"Lab-\"+lab_tag\n",
    "            if lab_tag in lab_scores:\n",
    "                lab_scores[lab_tag] += factor_score\n",
    "                lab_totals[lab_tag] += 1\n",
    "            else:\n",
    "                lab_scores[lab_tag] = factor_score\n",
    "                lab_totals[lab_tag] = 1\n",
    "        risk_scores.append((\"Encounter-\"+str(pt[2])+\": \"+factor, factor_score))\n",
    "\n",
    "    risk_scores.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    return risk_scores[:10]\n",
    "\n",
    "print 'Starting training..'\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print 'Run', run\n",
    "\n",
    "    perm = np.random.permutation(input_seqs_icd.shape[0])\n",
    "    rinput_seqs_icd = input_seqs_icd#[perm]\n",
    "    rinput_seqs_meds = input_seqs_meds#[perm]\n",
    "    rinput_seqs_labs = input_seqs_labs#[perm]\n",
    "    # rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = input_seqs_fullicd#[perm]\n",
    "    rlabels = labels#[perm]\n",
    "\n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    # train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    # valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    # test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = train_input_seqs_icd.shape[0]\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(size_average=False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print 'Epoch', (epoch+1)\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            # batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for i in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[i], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[i], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[i], vocabsize_labs)\n",
    "\n",
    "                icd_inputs = Variable(torch.from_numpy(icd_onehot).float())\n",
    "                med_inputs = Variable(torch.from_numpy(med_onehot).float())\n",
    "                lab_inputs = Variable(torch.from_numpy(lab_onehot).float())\n",
    "\n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "                # latent_inputs = Variable(torch.from_numpy(np.array(batch_latent[iter])).float())\n",
    "\n",
    "                targets = Variable(torch.from_numpy(batch_train_labels[i]).float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, med_inputs, lab_inputs, latent_inputs, None, force)\n",
    "\n",
    "                #print outputs[-1], targets\n",
    "                losses.append(criterion(outputs[-1], targets))\n",
    "\n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "\n",
    "        #print(epoch, epoch_loss)\n",
    "\n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "        print \"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions)\n",
    "\n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        # ICD_wise_corr = np.zeros(5)\n",
    "        # meds_wise_corr = np.zeros(5)\n",
    "        # labs_wise_corr = np.zeros(5)\n",
    "        # ICD_wise_tot = np.zeros(5)\n",
    "        # meds_wise_tot = np.zeros(5)\n",
    "        # labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "            \n",
    "            # ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            # meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            # labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print \"Test AUC_ROC: \", roc_auc_score(test_labels, predictions)\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        #fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        #pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open('roc_clout_cornn.p', 'wb'))\n",
    "        #actual_predictions = (predictions>0.5)*1\n",
    "        #print classification_report(test_labels, actual_predictions)\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print \"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs) \n",
    "\n",
    "# print \"Final testing and interpretations\"\n",
    "# predictions = np.zeros(len(test_input_seqs_icd))\n",
    "# for i in (range(len(test_input_seqs_icd))):\n",
    "# \ttest_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "# \ttest_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "# \ttest_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "# \ttest_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "# \ttest_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "# \ttest_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "# \ttest_score = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "# \tpredictions[i] = test_score\n",
    "# \ttop_risk_factors = get_factors(test_input_seqs_icd[i], test_input_seqs_meds[i], test_input_seqs_labs[i], model, test_score, test_input_seqs_fullicd[i]) \n",
    "# \tif (test_score>0.5):\n",
    "# \t\tinterpretation_file.write(\"ID: \" + str(i) + \" True label: \"+str(test_labels[i])+\"\\n\")\n",
    "# \t\tfor rf in top_risk_factors:\n",
    "# \t\t\tinterpretation_file.write(str(rf)+\"\\n\")\n",
    "# \t\tinterpretation_file.write(\"\\n\")\n",
    "\n",
    "interpretation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80438e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Graphs etc...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b66248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general instructions\n",
    "\n",
    "#2 or more patient encounters are required as the authors remove the last encounter while making predictions\n",
    "# patient mortality is the outcome label and aquired from mimic. \n",
    "#2825 out of 7537 patients died (37.9%)\n",
    "#Data set was divided into  train (5275/7537), validation (753/7537) and test (1509/7537)\n",
    "# Once the optimal parameters were selected, the model was retrained by combining train and validation (6028/7537)\n",
    "#\n",
    "#\n",
    "#********************** stars mean the data is not correct but the implemention functions.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b683be-1b9a-4270-952b-a8e5a9813fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from __future__ import division\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43c5b59-3e7a-4490-aa61-708d22d07470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed - only really necessary to compare results with each other, but can remove from the submission\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = \"~/Documents/DLH/mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "\n",
    "# define data - separated to show which modules load (some take much longer than others)\n",
    "#The commented out sections take a long time to load, and weren't immidiately useful at this stage\n",
    "#Can uncommment when relevant\n",
    "\n",
    "#TODO\n",
    "# - reorganize into a definition() after  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aae81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients = pd.read_csv(DATA_PATH + '/PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef884c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnosis = pd.read_csv(DATA_PATH + '/DIAGNOSES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab03c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Procedures = pd.read_csv(DATA_PATH + '/D_ICD_PROCEDURES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prescriptions = pd.read_csv(DATA_PATH + '/PRESCRIPTIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef15cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabEvents = pd.read_csv(DATA_PATH + '/LABEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8254e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabLabels = pd.read_csv(DATA_PATH + '/D_LABITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Admissions = pd.read_csv(DATA_PATH + '/ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a4c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcedureLabels = pd.read_csv(DATA_PATH + '/D_ITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea694043",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcuStays = pd.read_csv(DATA_PATH + '/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ce3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChartEvents = pd.read_csv(DATA_PATH + '/CHARTEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9207fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient inclusion - Only ICU patients with 2 or more visits are included. Patient IDs gathered\n",
    "#May be incorrect as the paper specifies only 7,537 patients in their list.\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#********************************************\n",
    "#Generate an ignore list for later ease of use.\n",
    "#Manually verified that all the included patients have 2 or more subject ids. Refine by HADM_ID? ICUSTAY_ID?\n",
    "\n",
    "def Patient_Inclusion(IcuStays):\n",
    "    IcuList = IcuStays.to_numpy()\n",
    "\n",
    "    unique, counts = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    print(\"Max encouters should equal 42?\", \"max counts: \", max(counts))\n",
    "    \n",
    "    AllPatients = np.array(list(zip(np.transpose(unique), np.transpose(counts))))\n",
    "    \n",
    "    PatientList = []\n",
    "    IgnoreList = []\n",
    "    \n",
    "    for i in AllPatients:\n",
    "        if i[1] >= 2:\n",
    "            PatientList.append(i)\n",
    "        else:\n",
    "            IgnoreList.append(i[0])\n",
    "            \n",
    "          \n",
    "    #uniquehadm, counts2 = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    \n",
    "            \n",
    "    #hadm_ID = IcuStays[['SUBJECT_ID', 'HADM_ID']]\n",
    "    \n",
    "    #IcuStay_ID = IcuStays[['SUBJECT_ID', 'ICUSTAY_ID']]\n",
    "    \n",
    "    #unique2= pd.DataFrame(PatientList, columns = [\"SUBJECT_ID\", \"Count\"])\n",
    "\n",
    "    #print(len(PatientList))\n",
    "    \n",
    "    #HadmIcuList = hadm_ID[hadm_ID.SUBJECT_ID.isin(unique2.SUBJECT_ID)]\n",
    "    #HadmIcuList = HadmIcuList.drop_duplicates(\"HADM_ID\")\n",
    "    #uniquehadm, counts2 = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    #hadm2 = np.array(list(zip(np.transpose(uniquehadm), np.transpose(counts2))))\n",
    "    #hadmlist, nothadm = [], []\n",
    "    #for i in hadm2:\n",
    "    #    if i[1] == 1:\n",
    "    #        hadmlist.append(i)\n",
    "    #    else:\n",
    "    #        nothadm.append(i[0])\n",
    "    \n",
    "    \n",
    "    #print(len(hadmlist))\n",
    "    \n",
    "    #print(hadmlist)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #HadmIcu = HadmIcuList[HadmIcuList.SUBJECT_ID.isin(hadm_ID.SUBJECT_ID)]\n",
    "    \n",
    "    #print(HadmIcuList)\n",
    "                \n",
    "\n",
    "    #print(len(IgnoreList))\n",
    "    return PatientList, IgnoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed2b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to create sequence lists like for Visit ID. \n",
    "\n",
    "def CreateList(n):\n",
    "    lst = []\n",
    "    for i in range(n):\n",
    "        lst.append(i)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b177839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max encouters should equal 42? max counts:  41\n"
     ]
    }
   ],
   "source": [
    "#To be added to main at the end\n",
    "\n",
    "\n",
    "#separate patient ids and count information\n",
    "PatientList, IgnoreList = Patient_Inclusion(IcuStays)\n",
    "PatientList = pd.DataFrame(PatientList, columns = ['ID', 'Count'])\n",
    "Patient_ID = PatientList['ID']\n",
    "Visits = PatientList['Count']\n",
    "VisitID = [] * len(Visits)\n",
    "count = 0\n",
    "for i in Visits:\n",
    "    temp = CreateList(i)\n",
    "    VisitID.append(temp)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6021b663-956f-47c5-87dc-ec28f2029ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data statistics - Ethnicity and visit statistics\n",
    "\n",
    "def Ethnicity_Statistics(Admissions):\n",
    "    white = 0\n",
    "    black = 0\n",
    "    hispanic = 0\n",
    "    asian = 0\n",
    "    other = 0\n",
    "    \n",
    "    #separate out ethnicity information\n",
    "    EthStatistics = Admissions[['SUBJECT_ID', 'ETHNICITY']]\n",
    "    IcuPatientEth = EthStatistics[EthStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "\n",
    "    #Not yet working.... General idea.\n",
    "    for i in IcuPatientEth.ETHNICITY:\n",
    "        if \"WHITE\" in i:\n",
    "            white += 1\n",
    "        if \"BLACK\" in i:\n",
    "            black += 1\n",
    "        if \"HISPANIC\" in i:\n",
    "            hispanic += 1\n",
    "        if \"ASIAN\"  in i:\n",
    "            asian += 1\n",
    "    \n",
    "        #I tried combining it, but the count was always more wrong...\n",
    "        if \"OTHER\" in i:\n",
    "            other += 1\n",
    "        if \"DECLINED\" in i:\n",
    "            other += 1\n",
    "        if \"NATIVE\" in i:\n",
    "            other += 1\n",
    "        if \"MIDDLE EASTERN\" in i:\n",
    "            other += 1\n",
    "        if \"MULTI\" in i:\n",
    "            other += 1\n",
    "        if \"UNABLE\" in i:\n",
    "            other += 1\n",
    "        if \"UNKNOWN\" in i:\n",
    "            other += 1\n",
    "        \n",
    "\n",
    "    print(white / len(IcuPatientEth), \"percent white\")\n",
    "    print(black / len(IcuPatientEth), \"percent black\")\n",
    "    print(hispanic / len(IcuPatientEth), \"percent hispanic\")\n",
    "    print(asian / len(IcuPatientEth), \"percent asian\")\n",
    "    print(other / len(IcuPatientEth), \"percent other\")\n",
    "\n",
    "    print(((white + black + hispanic + asian + other) / len(IcuPatientEth)), \"percent total\")\n",
    "    \n",
    "    #*****************************************************\n",
    "    #6 too many values. Need to sort that out.... maybe? it doesnt actually really affect anything...\n",
    "    print(len(IcuPatientEth))\n",
    "    print(white + black + asian + hispanic + other)\n",
    "    return white, black, hispanic, asian, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3159da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356938892061679 percent white\n",
      "0.10428326670474015 percent black\n",
      "0.03438035408338092 percent hispanic\n",
      "0.024785836664762993 percent asian\n",
      "0.10154197601370646 percent other\n",
      "1.0006853226727583 percent total\n",
      "8755\n",
      "8761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6441, 913, 301, 217, 889)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To be added to main at the end\n",
    "Ethnicity_Statistics(Admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b50730d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age statistics - Date of birth DOB is in patients, and admissions time is in admissions.\n",
    "# need to correlate both to find patients true ages\n",
    "#Sorting to ensure that each patient id is matched in case of misordering\n",
    "def Age_statistics(Patients):\n",
    "\n",
    "    #separate out date of birth information and sort\n",
    "    AgeStatistics = Patients[['SUBJECT_ID', 'DOB']]\n",
    "    IcuPatientDob = AgeStatistics[AgeStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientDob = IcuPatientDob.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #separate out admission date and sort\n",
    "    AdmStatistics = Admissions[['SUBJECT_ID', 'ADMITTIME']]\n",
    "    IcuPatientAdm = AdmStatistics[AdmStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientAdm = IcuPatientAdm.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #minus dates to find ages\n",
    "    #********************************************************\n",
    "    # not done yet. need to convert at least part of the date to an int for each dataframe\n",
    "    #Year and month should be sufficient, but year month date would increase accuracy. remove time.\n",
    "\n",
    "    \n",
    "    \n",
    "Age_statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b781bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex statistics\n",
    "def Gender_Statistics(Patients):\n",
    "    male = 0\n",
    "    female = 0\n",
    "    \n",
    "    SexStatistics = Patients[['SUBJECT_ID', 'GENDER']]\n",
    "    IcuPatientSex = SexStatistics[SexStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    \n",
    "    for i in IcuPatientSex.GENDER:\n",
    "        if i == \"M\":\n",
    "            male += 1\n",
    "        if i == \"F\":\n",
    "            female += 1\n",
    "            \n",
    "    print(male)\n",
    "    print(female)\n",
    "    \n",
    "    print(male / len(IcuPatientSex), \" percent male\")\n",
    "    print(female / len(IcuPatientSex), \"percent female\")\n",
    "    \n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b747c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n",
      "3812\n",
      "0.5645916619074814  percent male\n",
      "0.43540833809251855 percent female\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4943, 3812)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_Statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3b1979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "16.306453455168477\n",
      "7.5563203100575596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16821/1895454105.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  IcdStatistics.SEQ_NUM = IcdStatistics.SEQ_NUM.replace(np.nan,0).astype(int)\n",
      "/tmp/ipykernel_16821/1895454105.py:20: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  IcdGroup = IcuPatientIcd.groupby(\"SUBJECT_ID\")[\"SUBJECT_ID\", \"ICD9_CODE\"]\n"
     ]
    }
   ],
   "source": [
    "#ICD Code statistics\n",
    "def Icd_Codes(Diagnosis):\n",
    "    \n",
    "    #Isolate just the ICD codes and sequence of ICD codes\n",
    "    IcdStatistics = Diagnosis[['SUBJECT_ID', 'ICD9_CODE', 'SEQ_NUM']]\n",
    "    IcdStatistics.SEQ_NUM = IcdStatistics.SEQ_NUM.replace(np.nan,0).astype(int)\n",
    "    IcuPatientIcd = IcdStatistics[IcdStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    \n",
    "    #For overall ICD Statistics\n",
    "    EncounterTotal = IcuPatientIcd.drop_duplicates(['SUBJECT_ID', 'SEQ_NUM']).groupby('SUBJECT_ID')['SEQ_NUM'].count()\n",
    "    EncounterMax = max(EncounterTotal)\n",
    "    EncounterMean = np.mean(EncounterTotal)\n",
    "    EncounterSTD = np.std(EncounterTotal)\n",
    "    print(EncounterMax)\n",
    "    print(EncounterMean)\n",
    "    print(EncounterSTD)\n",
    "    \n",
    "    \n",
    "    #For ICD code groupings by patient id\n",
    "    IcdGroup = IcuPatientIcd.groupby(\"SUBJECT_ID\")[\"SUBJECT_ID\", \"ICD9_CODE\"]\n",
    "    \n",
    "\n",
    "    #for key, item in IcdGroup:\n",
    "        #print(IcdGroup.get_group(key), \"\\n\\n\")\n",
    "        #Data is ready but need to know the format before\n",
    "        #*******************************************************\n",
    "    \n",
    "Icd_Codes(Diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e09115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medication statistics\n",
    "def Medications():\n",
    "    #Need advice about medications for data science uses. In nursing, each medication is necessary \n",
    "    #however the authors mension removing duplicates per encounter. Should we only be keeping one medication\n",
    "    #type per encounter? This doesnt seem like a good idea, but I am not sure if that is what the author is \n",
    "    #implying. Or, should we keep one medication type per day?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde54247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab component statistics\n",
    "#The paper mentioned significant removal of duplicate patient information at this step (20% of data removed)\n",
    "#a total of 5,609,021 lab tests were removed as they were not used.\n",
    "\n",
    "def Laboratory(LabEvents):\n",
    "    #Only include the abnormal flags, and ignore the actual values.\n",
    "    LabStatistics = LabEvents[['SUBJECT_ID', 'ITEMID', 'FLAG']]\n",
    "    IcuPatientLab = LabStatistics[LabStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    \n",
    "    #Separate the lab events into all found in ICU patients. not sure if needed\n",
    "    IcuLabList = IcuPatientLab.drop_duplicates(['ITEMID'])\n",
    "    \n",
    "    #Gather only the abnormal flagged events\n",
    "    IcuPatientLab = IcuPatientLab.dropna().reset_index(drop = True)\n",
    "\n",
    "    #LabGroup = IcuPatientLab.groupby(\"SUBJECT_ID\")[\"SUBJECT_ID\", \"ITEMID\"]\n",
    "    \n",
    "    return IcuPatientLab\n",
    "    \n",
    "Laboratory(LabEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13517328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome label. Creates a list of 0's or 1's of length Patient_ID, where 1's indicate the patient passed away.\n",
    "\n",
    "\n",
    "def Mortality(Admissions, Patient_ID):\n",
    "    OutcomeFlag = Admissions[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    Outcome = OutcomeFlag[OutcomeFlag.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    Deceased = []\n",
    "\n",
    "\n",
    "    for i in Outcome.to_numpy():\n",
    "        if i[1] == 1:\n",
    "            Deceased.append(i[0])\n",
    "        \n",
    "    Deceased = np.array(Deceased)\n",
    "    Deceased = pd.DataFrame(Deceased, columns = ['Deceased']).to_numpy()\n",
    "\n",
    "    DeceasedFlag = []\n",
    "    for i in Patient_ID:\n",
    "        if i in Deceased:\n",
    "            DeceasedFlag.append(1)\n",
    "        else:\n",
    "            DeceasedFlag.append(0)\n",
    "            \n",
    "    return DeceasedFlag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35463316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model (The most important model to finish)\n",
    "\n",
    "#Source: https://github.com/subendhu19/CLOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clout outline photos to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Encoder (AE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681 284\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100):\n",
    "        super(AE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "\n",
    "        return [output_full, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print 'Epoch:', epoch\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "               \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print 'Epoch loss:', np.mean(losses)\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "model = AE(10,50,175)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print 'Pickled embedding weights. Shape:', np.array(emb_weights).shape\n",
    "pickle.dump(emb_weights, open('../full data/CAE/AE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# print \"Getting embeddings\"\n",
    "# outputs = []\n",
    "\n",
    "# for i in tqdm(range(0, ICD_data.shape[0], 50)):\n",
    "# \tICDbatch, Labbatch = ICD_data[i:i+50], Lab_data[i:i+50]\n",
    "# \toutputsbatch = model.get_encodings(ICDbatch, Labbatch).data.numpy()\n",
    "# \tfor ob in outputsbatch:\n",
    "# \t\toutputs.append(ob)\n",
    "\n",
    "# pickle.dump(np.array(outputs), open('../full data/CAE/AE_Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Auto Encoder (CAE)\n",
    "\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681 284\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, embsize=100, lamb=0.01):\n",
    "        super(CAE, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.embsize = embsize\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.emb = nn.Linear(vocabsize_icd + vocabsize_meds + vocabsize_labs, self.embsize)\n",
    "\n",
    "        self.out = nn.Linear(self.embsize, vocabsize_icd + vocabsize_meds + vocabsize_labs)\n",
    "\n",
    "        self.reconloss = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab):\n",
    "\n",
    "        input_full = torch.cat((input_icd, input_med, input_lab),1)\n",
    "        input_onlyicd = torch.cat((input_icd, Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlymed = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), input_med, Variable(torch.zeros(input_lab.size(0), input_lab.size(1)).float())), 1)\n",
    "        input_onlylab = torch.cat((Variable(torch.zeros(input_icd.size(0), input_icd.size(1)).float()), Variable(torch.zeros(input_med.size(0), input_med.size(1)).float()), input_lab), 1)\n",
    "\n",
    "        hidden_full = F.relu(self.emb(input_full))\n",
    "        hidden_onlyicd = F.relu(self.emb(input_onlyicd))\n",
    "        hidden_onlymed = F.relu(self.emb(input_onlymed))\t\n",
    "        hidden_onlylab = F.relu(self.emb(input_onlylab))\t\t \n",
    "\n",
    "        output_full = F.relu(self.out(hidden_full))\n",
    "        output_onlyicd = F.relu(self.out(hidden_onlyicd))\n",
    "        output_onlymed = F.relu(self.out(hidden_onlymed))\n",
    "        output_onlylab = F.relu(self.out(hidden_onlylab))\t\n",
    "\n",
    "        return [output_full, output_onlyicd, output_onlymed, output_onlylab, hidden_onlyicd, hidden_onlymed, hidden_onlylab, hidden_full]\n",
    "\n",
    "    def get_encodings(self, ICD_data, Med_data, Lab_data):\n",
    "        return self.forward(Variable(torch.from_numpy(ICD_data).float()), Variable(torch.from_numpy(Med_data).float()), Variable(torch.from_numpy(Lab_data).float()))[-1]\n",
    "\n",
    "    def correlation_coef(self, x, y):\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "\n",
    "        cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "        return cost\n",
    "\n",
    "    def joint_cumulant_by_var(self, x, y, z):\t\n",
    "        vx = x - torch.mean(x)\n",
    "        vy = y - torch.mean(y)\n",
    "        vz = z - torch.mean(z)\n",
    "\n",
    "        cost = torch.sum(vx * vy * vz) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) * torch.sqrt(torch.sum(vz ** 2)))\n",
    "        return cost\n",
    "\n",
    "        # e_xyz = torch.mean(x * y * z)\n",
    "        # e_xy = torch.mean(x * y)\n",
    "        # e_yz = torch.mean(y * z)\n",
    "        # e_xz = torch.mean(x * z)\n",
    "        # e_x = torch.mean(x)\n",
    "        # e_y = torch.mean(y)\n",
    "        # e_z = torch.mean(z)\n",
    "\n",
    "        # kappa = e_xyz - (e_xy * e_z) - (e_xz * e_y) - (e_yz * e_x) + (2*e_x*e_y*e_z) \n",
    "\n",
    "\n",
    "    def fit(self, ICDs, Meds, Labs):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
    "\n",
    "        prev_loss = 1000\n",
    "        for epoch in range(self.epochs):\n",
    "            print 'Epoch:', epoch\n",
    "\n",
    "            perm = np.random.permutation(ICDs.shape[0])\n",
    "            ICDs = ICDs[perm]\n",
    "            Meds = Meds[perm]\n",
    "            Labs = Labs[perm]\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i in range(0, ICDs.shape[0], self.batchsize):\n",
    "                ICDbatch, Medbatch, Labbatch = ICDs[i:i+self.batchsize], Meds[i:i+self.batchsize], Labs[i:i+self.batchsize]\n",
    "                ICDbatchvar, Medbatchvar, Labbatchvar = Variable(torch.from_numpy(ICDbatch).float()), Variable(torch.from_numpy(Medbatch).float()), Variable(torch.from_numpy(Labbatch).float())\n",
    "\n",
    "                outputs = self.forward(ICDbatchvar, Medbatchvar, Labbatchvar)\n",
    "\n",
    "                loss_recon = self.reconloss(outputs[0], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[1], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) \\\n",
    "                        + self.reconloss(outputs[2], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1)) + self.reconloss(outputs[3], torch.cat((ICDbatchvar, Medbatchvar, Labbatchvar),1))\n",
    "\n",
    "                loss_cr = self.joint_cumulant_by_var(outputs[4], outputs[5], outputs[6])\n",
    "\n",
    "                loss = loss_recon - (self.lamb*loss_cr)\n",
    "\n",
    "                losses.append(loss.data[0])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                # print 'recon loss:', loss_recon.data[0], 'loss_cr:', loss_cr.data[0]\n",
    "\n",
    "            print 'Epoch loss:', np.mean(losses)\n",
    "\n",
    "            if abs(np.mean(losses) - prev_loss) < 0.00005:\n",
    "                break\n",
    "\n",
    "            prev_loss = np.mean(losses)\n",
    "\n",
    "\n",
    "model = CAE(10,50,175,0.01)\n",
    "ICD_data = pickle.load(open('../full data/CAE/CAEEntries.3digitICD9','r'))\n",
    "Med_data = pickle.load(open('../full data/CAE/CAEEntries.meds','r'))\n",
    "Lab_data = pickle.load(open('../full data/CAE/CAEEntries.abnlabs','r'))\n",
    "model.fit(ICD_data, Med_data, Lab_data)\n",
    "\n",
    "emb_weights = model._modules['emb'].weight.data.numpy().T\n",
    "print 'Pickled embedding weights. Shape:', np.array(emb_weights).shape\n",
    "pickle.dump(emb_weights, open('../full data/CAE/CAE_embedding_weights.npy', 'wb'))\n",
    "\n",
    "# outputs = model.get_encodings(ICD_data, Med_data, Lab_data)\n",
    "# print np.array(outputs).shape\n",
    "# pickle.dump(outputs, open('../full data/CAE/Embeddings', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb_icd = nn.Linear(vocabsize_icd, embsize_icd)\n",
    "        self.emb_meds = nn.Linear(vocabsize_meds, embsize_meds)\n",
    "        self.emb_labs = nn.Linear(vocabsize_labs, embsize_labs)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_icd)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        input_icd = self.emb_icd(input_icd)\n",
    "        input_med = self.emb_meds(input_med)\n",
    "        input_lab = self.emb_labs(input_lab)\n",
    "\n",
    "        inputs = F.relu(torch.cat((input_icd, input_med, input_lab, input_latent),1))\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_icd, input_med, input_lab, input_latent):\n",
    "        out, hid = self.forward(input_icd, input_med, input_lab, input_latent, None)\n",
    "        return self.sig(out[-1]).data[0]\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Coq tactic prediction')\n",
    "parser.add_argument('--emb_weights', type=str, default='../full data/CAE/CAE_embedding_weights.npy',\n",
    "                    help='location of the embedding weight file (default: ../full data/CAE/CAE_embedding_weights.npy)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_icd = 50\n",
    "embsize_meds = 75\n",
    "embsize_labs = 50\n",
    "embsize_latent = 175\n",
    "embsize = embsize_icd + embsize_labs + embsize_meds + embsize_latent\n",
    "\n",
    "input_seqs_icd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.3digitICD9.seqs')))\n",
    "input_seqs_meds = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.meds.seqs')))\n",
    "input_seqs_labs = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.abnlabs.seqs')))\n",
    "# input_seqs_latent = np.array(pickle.load(open('../full data/CAE/Embeddings.seqs')))\n",
    "latent_weights = pickle.load(open(args.emb_weights))\n",
    "\n",
    "input_seqs_fullicd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.seqs')))\n",
    "\n",
    "icditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.types', 'rb'))\n",
    "meditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.meds.types', 'rb'))\n",
    "labitems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.abnlabs.types', 'rb'))\n",
    "\n",
    "model_name = 'AE'\n",
    "if args.emb_weights == '../full data/CAE/CAE_embedding_weights.npy':\n",
    "    model_name = 'CAE'\n",
    "\n",
    "interpretation_file = open(\"RNN_CLatent_Interpretations_\" + model_name + \".txt\", 'w')\n",
    "\n",
    "# overall_risk_factor_file = open(\"risk_factors_averaged.txt\", \"w\")\n",
    "\n",
    "labnames = {}\n",
    "lab_dict_file = open('D_LABITEMS.csv', 'r')\n",
    "lab_dict_file.readline()\n",
    "for line in lab_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    labnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "lab_dict_file.close()\n",
    "\n",
    "icdnames = {}\n",
    "icd_dict_file = open('D_ICD_DIAGNOSES.csv', 'r')\n",
    "icd_dict_file.readline()\n",
    "for line in icd_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    icdnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "icd_dict_file.close()\n",
    "\n",
    "icd_scores = {}\n",
    "med_scores = {}\n",
    "lab_scores = {}\n",
    "\n",
    "icd_totals = {}\n",
    "med_totals = {}\n",
    "lab_totals = {}\n",
    "\n",
    "def get_ICD(icd):\n",
    "    ret_str = \"\"\n",
    "    icd_str = icditems.keys()[icditems.values().index(icd)]\n",
    "    actual_key = icd_str.replace(\".\", \"\")[2:]\n",
    "    if actual_key in icdnames:\n",
    "        ret_str = icdnames[actual_key]\n",
    "    else:\n",
    "        ret_str = icd_str\n",
    "    return ret_str\n",
    "\n",
    "def get_med(med):\n",
    "    ret_str = meditems.keys()[meditems.values().index(med)]\n",
    "    return ret_str\n",
    "\n",
    "def get_lab(lab):\n",
    "    ret_str = labnames[labitems.keys()[labitems.values().index(lab)]]\n",
    "    return ret_str\n",
    "\n",
    "print 'Data loaded..'\n",
    "\n",
    "labels = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.morts')))\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "print 'Data prepared..'\n",
    "\n",
    "def convert_to_one_hot(code_seqs, len):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))\n",
    "\n",
    "def get_factors(icd_seq, med_seq, lab_seq, model, actual_score, full_icd):\n",
    "    potential_test_data = []\n",
    "\n",
    "    for seq in range(len(icd_seq)):\n",
    "        for i in range(len(icd_seq[seq])):\n",
    "            potential_test_data.append((\"icd\", full_icd[seq][i], seq, icd_seq[:seq]+[icd_seq[seq][:i] + icd_seq[seq][i+1:]]+icd_seq[seq+1:], med_seq, lab_seq))\n",
    "    for seq in range(len(med_seq)):\n",
    "        for i in range(len(med_seq[seq])):\n",
    "            potential_test_data.append((\"med\", med_seq[seq][i], seq, icd_seq, med_seq[:seq]+[med_seq[seq][:i]+med_seq[seq][i+1:]]+med_seq[seq+1:], lab_seq))\n",
    "    for seq in range(len(lab_seq)):\n",
    "        for i in range(len(lab_seq[seq])):\n",
    "            potential_test_data.append((\"lab\", lab_seq[seq][i], seq, icd_seq, med_seq, lab_seq[:seq]+[lab_seq[seq][:i] + lab_seq[seq][i+1:]]+lab_seq[seq+1:]))\n",
    "\n",
    "    risk_scores = []\n",
    "\n",
    "    for pt in potential_test_data:\n",
    "        test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(pt[3], vocabsize_icd)).float())\n",
    "        test_input_med = Variable(torch.from_numpy(convert_to_one_hot(pt[4], vocabsize_meds)).float())\n",
    "        test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(pt[5], vocabsize_labs)).float())\n",
    "\n",
    "        latent_inputs_oh = np.concatenate((convert_to_one_hot(pt[3], vocabsize_icd), convert_to_one_hot(pt[4], vocabsize_meds), convert_to_one_hot(pt[5], vocabsize_labs)), 1)\n",
    "        latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "        latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "        factor_score = actual_score - model.predict(test_input_icd, test_input_med, test_input_lab, latent_inputs)\n",
    "        factor = \"\"\n",
    "        if pt[0] == 'icd':\n",
    "            icd_tag = get_ICD(pt[1])\n",
    "            factor = \"ICD-\"+icd_tag\n",
    "            if icd_tag in icd_scores:\n",
    "                icd_scores[icd_tag] += factor_score\n",
    "                icd_totals[icd_tag] += 1\n",
    "            else:\n",
    "                icd_scores[icd_tag] = factor_score\n",
    "                icd_totals[icd_tag] = 1\n",
    "        elif pt[0] == 'med':\n",
    "            med_tag = get_med(pt[1])\n",
    "            factor = \"Med-\"+med_tag\n",
    "            if med_tag in med_scores:\n",
    "                med_scores[med_tag] += factor_score\n",
    "                med_totals[med_tag] += 1\n",
    "            else:\n",
    "                med_scores[med_tag] = factor_score\n",
    "                med_totals[med_tag] = 1\n",
    "        else:\n",
    "            lab_tag = get_lab(pt[1])\n",
    "            factor = \"Lab-\"+lab_tag\n",
    "            if lab_tag in lab_scores:\n",
    "                lab_scores[lab_tag] += factor_score\n",
    "                lab_totals[lab_tag] += 1\n",
    "            else:\n",
    "                lab_scores[lab_tag] = factor_score\n",
    "                lab_totals[lab_tag] = 1\n",
    "        risk_scores.append((\"Encounter-\"+str(pt[2])+\": \"+factor, factor_score))\n",
    "\n",
    "    risk_scores.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    return risk_scores[:10]\n",
    "\n",
    "print 'Starting training..'\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print 'Run', run\n",
    "\n",
    "    perm = np.random.permutation(input_seqs_icd.shape[0])\n",
    "    rinput_seqs_icd = input_seqs_icd#[perm]\n",
    "    rinput_seqs_meds = input_seqs_meds#[perm]\n",
    "    rinput_seqs_labs = input_seqs_labs#[perm]\n",
    "    # rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = input_seqs_fullicd#[perm]\n",
    "    rlabels = labels#[perm]\n",
    "\n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    # train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    # valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    # test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = train_input_seqs_icd.shape[0]\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(size_average=False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print 'Epoch', (epoch+1)\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            # batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for i in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[i], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[i], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[i], vocabsize_labs)\n",
    "\n",
    "                icd_inputs = Variable(torch.from_numpy(icd_onehot).float())\n",
    "                med_inputs = Variable(torch.from_numpy(med_onehot).float())\n",
    "                lab_inputs = Variable(torch.from_numpy(lab_onehot).float())\n",
    "\n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "                # latent_inputs = Variable(torch.from_numpy(np.array(batch_latent[iter])).float())\n",
    "\n",
    "                targets = Variable(torch.from_numpy(batch_train_labels[i]).float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, med_inputs, lab_inputs, latent_inputs, None, force)\n",
    "\n",
    "                #print outputs[-1], targets\n",
    "                losses.append(criterion(outputs[-1], targets))\n",
    "\n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "\n",
    "        #print(epoch, epoch_loss)\n",
    "\n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "        print \"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions)\n",
    "\n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        # ICD_wise_corr = np.zeros(5)\n",
    "        # meds_wise_corr = np.zeros(5)\n",
    "        # labs_wise_corr = np.zeros(5)\n",
    "        # ICD_wise_tot = np.zeros(5)\n",
    "        # meds_wise_tot = np.zeros(5)\n",
    "        # labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "            \n",
    "            # ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            # meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            # labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print \"Test AUC_ROC: \", roc_auc_score(test_labels, predictions)\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        #fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        #pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open('roc_clout_cornn.p', 'wb'))\n",
    "        #actual_predictions = (predictions>0.5)*1\n",
    "        #print classification_report(test_labels, actual_predictions)\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print \"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs) \n",
    "\n",
    "# print \"Final testing and interpretations\"\n",
    "# predictions = np.zeros(len(test_input_seqs_icd))\n",
    "# for i in (range(len(test_input_seqs_icd))):\n",
    "# \ttest_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "# \ttest_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "# \ttest_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "# \ttest_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "# \ttest_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "# \ttest_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "# \ttest_score = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "# \tpredictions[i] = test_score\n",
    "# \ttop_risk_factors = get_factors(test_input_seqs_icd[i], test_input_seqs_meds[i], test_input_seqs_labs[i], model, test_score, test_input_seqs_fullicd[i]) \n",
    "# \tif (test_score>0.5):\n",
    "# \t\tinterpretation_file.write(\"ID: \" + str(i) + \" True label: \"+str(test_labels[i])+\"\\n\")\n",
    "# \t\tfor rf in top_risk_factors:\n",
    "# \t\t\tinterpretation_file.write(str(rf)+\"\\n\")\n",
    "# \t\tinterpretation_file.write(\"\\n\")\n",
    "\n",
    "interpretation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80438e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Graphs etc...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

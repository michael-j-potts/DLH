{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54b66248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general instructions\n",
    "\n",
    "#2 or more patient encounters are required as the authors remove the last encounter while making predictions\n",
    "# patient mortality is the outcome label and aquired from mimic. \n",
    "#2825 out of 7537 patients died (37.9%)\n",
    "#Data set was divided into  train (5275/7537), validation (753/7537) and test (1509/7537)\n",
    "# Once the optimal parameters were selected, the model was retrained by combining train and validation (6028/7537)\n",
    "#\n",
    "#\n",
    "#********************** stars mean the data is not correct but the implemention functions.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7b683be-1b9a-4270-952b-a8e5a9813fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from __future__ import division\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d43c5b59-3e7a-4490-aa61-708d22d07470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed - only really necessary to compare results with each other, but can remove from the submission\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = \"~/Documents/DLH 598/mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "\n",
    "# define data - separated to show which modules load (some take much longer than others)\n",
    "#The commented out sections take a long time to load, and weren't immidiately useful at this stage\n",
    "#Can uncommment when relevant\n",
    "\n",
    "#TODO\n",
    "# - reorganize into a definition() after  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5aae81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients = pd.read_csv(DATA_PATH + '/PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aef884c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnosis = pd.read_csv(DATA_PATH + '/DIAGNOSES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab03c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Procedures = pd.read_csv(DATA_PATH + '/D_ICD_PROCEDURES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ba977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prescriptions = pd.read_csv(DATA_PATH + '/PRESCRIPTIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ef15cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabEvents = pd.read_csv(DATA_PATH + '/LABEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8254e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabLabels = pd.read_csv(DATA_PATH + '/D_LABITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c3e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Admissions = pd.read_csv(DATA_PATH + '/ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0a4c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcedureLabels = pd.read_csv(DATA_PATH + '/D_ITEMS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea694043",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcuStays = pd.read_csv(DATA_PATH + '/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1ce3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChartEvents = pd.read_csv(DATA_PATH + '/CHARTEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9207fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient inclusion - Only ICU patients with 2 or more visits are included. Patient IDs gathered\n",
    "#May be incorrect as the paper specifies only 7,537 patients in their list.\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#********************************************\n",
    "#Generate an ignore list for later ease of use.\n",
    "#Manually verified that all the included patients have 2 or more subject ids. Refine by HADM_ID? ICUSTAY_ID?\n",
    "\n",
    "def Patient_Inclusion(IcuStays):\n",
    "    IcuList = IcuStays.to_numpy()\n",
    "\n",
    "    unique, counts = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    #IcuStayList = EthStatistics[EthStatistics.SUBJECT_ID.isin(Patient_ID)]\n",
    "    \n",
    "    print(\"Max encouters should equal 42?\", \"max counts: \", max(counts))\n",
    "    \n",
    "    AllPatients = np.array(list(zip(np.transpose(unique), np.transpose(counts))))\n",
    "    \n",
    "    PatientList = []\n",
    "    IgnoreList = []\n",
    "    \n",
    "    for i in AllPatients:\n",
    "        if i[1] >= 2:\n",
    "            PatientList.append(i)\n",
    "        else:\n",
    "            IgnoreList.append(i[0])\n",
    "            \n",
    "            \n",
    "    uniquehadm, counts2 = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    \n",
    "            \n",
    "    hadm_ID = IcuStays[['SUBJECT_ID', 'HADM_ID']]\n",
    "    \n",
    "    IcuStay_ID = IcuStays[['SUBJECT_ID', 'ICUSTAY_ID']]\n",
    "    \n",
    "    unique2= pd.DataFrame(PatientList, columns = [\"SUBJECT_ID\", \"Count\"])\n",
    "\n",
    "    #print(len(PatientList))\n",
    "    \n",
    "    HadmIcuList = hadm_ID[hadm_ID.SUBJECT_ID.isin(unique2.SUBJECT_ID)]\n",
    "    HadmIcuList = HadmIcuList.drop_duplicates(\"HADM_ID\")\n",
    "    uniquehadm, counts2 = np.unique(IcuList[1:, 1], return_counts=True)\n",
    "    \n",
    "    hadm2 = np.array(list(zip(np.transpose(uniquehadm), np.transpose(counts2))))\n",
    "    hadmlist, nothadm = [], []\n",
    "    for i in hadm2:\n",
    "        if i[1] == 1:\n",
    "            hadmlist.append(i)\n",
    "        else:\n",
    "            nothadm.append(i[0])\n",
    "    \n",
    "    \n",
    "    #print(len(hadmlist))\n",
    "    \n",
    "    #print(hadmlist)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #HadmIcu = HadmIcuList[HadmIcuList.SUBJECT_ID.isin(hadm_ID.SUBJECT_ID)]\n",
    "    \n",
    "    #print(HadmIcuList)\n",
    "                \n",
    "\n",
    "    #print(len(IgnoreList))\n",
    "    return PatientList, IgnoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b177839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max encouters should equal 42? max counts:  41\n"
     ]
    }
   ],
   "source": [
    "#To be added to main at the end\n",
    "\n",
    "\n",
    "#separate patient ids and count information\n",
    "PatientList, IgnoreList = Patient_Inclusion(IcuStays)\n",
    "PatientList = pd.DataFrame(PatientList, columns = ['ID', 'Count'])\n",
    "Patient_ID = PatientList['ID']\n",
    "Visits = PatientList['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6021b663-956f-47c5-87dc-ec28f2029ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data statistics - Ethnicity and visit statistics\n",
    "\n",
    "def Ethnicity_Statistics(Admissions):\n",
    "    white = 0\n",
    "    black = 0\n",
    "    hispanic = 0\n",
    "    asian = 0\n",
    "    other = 0\n",
    "    \n",
    "    #separate out ethnicity information\n",
    "    EthStatistics = Admissions[['SUBJECT_ID', 'ETHNICITY']]\n",
    "    IcuPatientEth = EthStatistics[EthStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "\n",
    "    #Not yet working.... General idea.\n",
    "    for i in IcuPatientEth.ETHNICITY:\n",
    "        if \"WHITE\" in i:\n",
    "            white += 1\n",
    "        if \"BLACK\" in i:\n",
    "            black += 1\n",
    "        if \"HISPANIC\" in i:\n",
    "            hispanic += 1\n",
    "        if \"ASIAN\"  in i:\n",
    "            asian += 1\n",
    "    \n",
    "        #I tried combining it, but the count was always more wrong...\n",
    "        if \"OTHER\" in i:\n",
    "            other += 1\n",
    "        if \"DECLINED\" in i:\n",
    "            other += 1\n",
    "        if \"NATIVE\" in i:\n",
    "            other += 1\n",
    "        if \"MIDDLE EASTERN\" in i:\n",
    "            other += 1\n",
    "        if \"MULTI\" in i:\n",
    "            other += 1\n",
    "        if \"UNABLE\" in i:\n",
    "            other += 1\n",
    "        if \"UNKNOWN\" in i:\n",
    "            other += 1\n",
    "        \n",
    "\n",
    "    print(white / len(IcuPatientEth), \"percent white\")\n",
    "    print(black / len(IcuPatientEth), \"percent black\")\n",
    "    print(hispanic / len(IcuPatientEth), \"percent hispanic\")\n",
    "    print(asian / len(IcuPatientEth), \"percent asian\")\n",
    "    print(other / len(IcuPatientEth), \"percent other\")\n",
    "\n",
    "    print(((white + black + hispanic + asian + other) / len(IcuPatientEth)), \"percent total\")\n",
    "    \n",
    "    #*****************************************************\n",
    "    #6 too many values. Need to sort that out.... maybe? it doesnt actually really affect anything...\n",
    "    print(len(IcuPatientEth))\n",
    "    print(white + black + asian + hispanic + other)\n",
    "    return white, black, hispanic, asian, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3159da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356938892061679 percent white\n",
      "0.10428326670474015 percent black\n",
      "0.03438035408338092 percent hispanic\n",
      "0.024785836664762993 percent asian\n",
      "0.10154197601370646 percent other\n",
      "1.0006853226727583 percent total\n",
      "8755\n",
      "8761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6441, 913, 301, 217, 889)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To be added to main at the end\n",
    "Ethnicity_Statistics(Admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b50730d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age statistics - Date of birth DOB is in patients, and admissions time is in admissions.\n",
    "# need to correlate both to find patients true ages\n",
    "#Sorting to ensure that each patient id is matched in case of misordering\n",
    "def Age_statistics(Patients):\n",
    "\n",
    "    #separate out date of birth information and sort\n",
    "    AgeStatistics = Patients[['SUBJECT_ID', 'DOB']]\n",
    "    IcuPatientDob = AgeStatistics[AgeStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientDob = IcuPatientDob.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #separate out admission date and sort\n",
    "    AdmStatistics = Admissions[['SUBJECT_ID', 'ADMITTIME']]\n",
    "    IcuPatientAdm = AdmStatistics[AdmStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    IcuPatientAdm = IcuPatientAdm.sort_values(by=['SUBJECT_ID']).reset_index(drop = True)\n",
    "    \n",
    "    #minus dates to find ages\n",
    "    #********************************************************\n",
    "    # not done yet. need to convert at least part of the date to an int for each dataframe\n",
    "    #Year and month should be sufficient, but year month date would increase accuracy. remove time.\n",
    "    #PatientAges = IcuPatientAdm.ADMITTIME - IcuPatientDob.DOB\n",
    "\n",
    "    \n",
    "    \n",
    "Age_statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13b781bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex statistics\n",
    "def Gender_Statistics(Patients):\n",
    "    male = 0\n",
    "    female = 0\n",
    "    \n",
    "    SexStatistics = Patients[['SUBJECT_ID', 'GENDER']]\n",
    "    IcuPatientSex = SexStatistics[SexStatistics.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "    \n",
    "    for i in IcuPatientSex.GENDER:\n",
    "        if i == \"M\":\n",
    "            male += 1\n",
    "        if i == \"F\":\n",
    "            female += 1\n",
    "            \n",
    "    print(male)\n",
    "    print(female)\n",
    "    \n",
    "    print(male / len(IcuPatientSex), \" percent male\")\n",
    "    print(female / len(IcuPatientSex), \"percent female\")\n",
    "    \n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b747c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n",
      "3812\n",
      "0.5645916619074814  percent male\n",
      "0.43540833809251855 percent female\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4943, 3812)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_Statistics(Patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3b1979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1.0\n",
      "1      2.0\n",
      "2      3.0\n",
      "3      4.0\n",
      "4      5.0\n",
      "5      6.0\n",
      "6      7.0\n",
      "7      8.0\n",
      "8      9.0\n",
      "9     10.0\n",
      "10    11.0\n",
      "11    12.0\n",
      "12    13.0\n",
      "13    14.0\n",
      "14     1.0\n",
      "15     2.0\n",
      "16     3.0\n",
      "17     4.0\n",
      "18     5.0\n",
      "19     6.0\n",
      "Name: SEQ_NUM, dtype: float64\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m IcdGroup:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(IcdGroup\u001b[38;5;241m.\u001b[39mget_group(key), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mIcd_Codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDiagnosis\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36mIcd_Codes\u001b[0;34m(Diagnosis)\u001b[0m\n\u001b[1;32m      5\u001b[0m IcdStatistics \u001b[38;5;241m=\u001b[39m Diagnosis[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICD9_CODE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQ_NUM\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(IcdStatistics\u001b[38;5;241m.\u001b[39mSEQ_NUM\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m IcdStatistics\u001b[38;5;241m.\u001b[39mSEQ_NUM \u001b[38;5;241m=\u001b[39m \u001b[43mIcdStatistics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEQ_NUM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(IcdStatistics\u001b[38;5;241m.\u001b[39mSEQ_NUM\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m      9\u001b[0m IcuPatientIcd \u001b[38;5;241m=\u001b[39m IcdStatistics[IcdStatistics\u001b[38;5;241m.\u001b[39mSUBJECT_ID\u001b[38;5;241m.\u001b[39misin(Patient_ID)]\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5920\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5913\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   5914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   5915\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m   5916\u001b[0m     ]\n\u001b[1;32m   5918\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5919\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 5920\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5923\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:419\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:304\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:580\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 580\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    583\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1292\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1292\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1237\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1148\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot astype a timedelta from [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] to [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(dtype, np\u001b[38;5;241m.\u001b[39minteger):\n\u001b[0;32m-> 1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mastype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1151\u001b[0m \n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# work around NumPy brokenness, #1987\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(dtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1193\u001b[0m, in \u001b[0;36mastype_float_to_int_nansafe\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(values)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "#ICD Code statistics\n",
    "def Icd_Codes(Diagnosis):\n",
    "    \n",
    "    #Isolate just the ICD codes and sequence of ICD codes\n",
    "    IcdStatistics = Diagnosis[['SUBJECT_ID', 'ICD9_CODE', 'SEQ_NUM']]\n",
    "    print(IcdStatistics.SEQ_NUM.head(20))\n",
    "    IcdStatistics.SEQ_NUM = IcdStatistics.SEQ_NUM.astype(int)\n",
    "    print(IcdStatistics.SEQ_NUM.head(20))\n",
    "    IcuPatientIcd = IcdStatistics[IcdStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "    \n",
    "    #For overall ICD Statistics\n",
    "    EncounterTotal = IcuPatientIcd.drop_duplicates(['SUBJECT_ID', 'SEQ_NUM']).groupby('SUBJECT_ID')['SEQ_NUM'].count()\n",
    "    EncounterMax = max(EncounterTotal)\n",
    "    EncounterMean = np.mean(EncounterTotal)\n",
    "    EncounterSTD = np.std(EncounterTotal)\n",
    "    print(EncounterMax)\n",
    "    print(EncounterMean)\n",
    "    print(EncounterSTD)\n",
    "    \n",
    "    \n",
    "    #For ICD code groupings by patient id\n",
    "    IcdGroup = IcuPatientIcd.groupby(\"SUBJECT_ID\")[\"ICD9_CODE\"]\n",
    "\n",
    "    for key, item in IcdGroup:\n",
    "        print(IcdGroup.get_group(key), \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "Icd_Codes(Diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medication statistics\n",
    "def Medications():\n",
    "    #Need advice about medications for data science uses. In nursing, each medication is necessary \n",
    "    #however the authors mension removing duplicates per encounter. Should we only be keeping one medication\n",
    "    #type per encounter? This doesnt seem like a good idea, but I am not sure if that is what the author is \n",
    "    #implying. Or, should we keep one medication type per day?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde54247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab component statistics\n",
    "#The paper mentioned significant removal of duplicate patient information at this step (20% of data removed)\n",
    "#a total of 5,609,021 lab tests were removed as they were not used.\n",
    "\n",
    "def Laboratory():\n",
    "    #Only include the abnormal flags, and ignore the actual values.\n",
    "    LabStatistics = LabEvents[['SUBJECT_ID', 'ITEMID', 'FLAG']]\n",
    "    LabStatistics.SEQ_NUM = LabStatistics.SEQ_NUM\n",
    "    IcuPatientLab = LabStatistics[LabStatistics.SUBJECT_ID.isin(Patient_ID)].reset_index(drop = True)\n",
    "\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13517328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome label = patient mortatility. Flag found in Admissions?\n",
    "\n",
    "#Right idea, may not be correct\n",
    "#**************************************************\n",
    "\n",
    "OutcomeFlag = Admissions[['SUBJECT_ID', 'HOSPITAL_EXPIRE_FLAG']]\n",
    "Outcome = OutcomeFlag[OutcomeFlag.SUBJECT_ID.isin(Patient_ID)].drop_duplicates('SUBJECT_ID').reset_index(drop = True)\n",
    "Deceased = []\n",
    "\n",
    "\n",
    "for i in Outcome.to_numpy():\n",
    "    if i[1] == 1:\n",
    "        Deceased.append(i[0])\n",
    "        \n",
    "Deceased = np.array(Deceased)\n",
    "pd.DataFrame(Deceased, columns = ['Deceased'])  \n",
    "\n",
    "\n",
    "print(Deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain\n",
    "#just the skeleton from the assignment. Complete but not yet tested or functional. Need to mold data to this first.\n",
    "#**************************************************************\n",
    "\n",
    "# Not implemented but may be better than the course work assignment model!!! That being said, there is no attention\n",
    "# so it may just be an RNN instead of true Retain\n",
    "#***************************************************************************\n",
    "# Alternate model from github forr CLOUT: https://github.com/subendhu19/CLOUT/blob/master/rnn_icd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate alpha attention\n",
    "\n",
    "class AlphaAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dimension):\n",
    "        super().__init__()\n",
    "        self.Alpha_Attention = nn.Linear(hidden_dimension, 1)\n",
    "        \n",
    "    def forward(self, gi):\n",
    "        x = self.Alpha_Attention(gi)\n",
    "        softmax = torch.nn.Softmax(dim = 1)\n",
    "        x = softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fecd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate beta attention\n",
    "\n",
    "class BetaAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_dimension):\n",
    "        super().__init__()\n",
    "        self.Beta_Attention = nn.Linear(hidden_dimension, hidden_dimension)\n",
    "        \n",
    "    def forward(self, hi):\n",
    "        x = self.Beta_Attention(hi)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum the alpha and beta attention together.\n",
    "\n",
    "def AttentionSum(alpha, beta, reverse_v, reverse_masks):\n",
    "    rev_masks = torch.unsqueeze(torch.max(reverse_masks, dim = 2)[0], dim = -1)\n",
    "    c = alpha * beta * (rev_v * reverse_masks)\n",
    "    c = torch.sum(c, dim = 1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum embeddings with the mask\n",
    "\n",
    "def SumEmbeddingsWithMask(x, masks):\n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the retain model\n",
    "#Diagnosis codes are a dictionary of diagnosis codes with a numerical value of its order following\n",
    "#ex. diag_101 : 0, diag_103 : 1, diag_187 : 2 ....\n",
    "\n",
    "class Retain(nn.Module):\n",
    "    def __init__(self, diagnosis_codes, embedding_dimension = 128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(diagnosis_codes, embedding_dimension)\n",
    "        self.rnn_alpha = nn.GRU(embedding_dimension, embedding_dimension, batch_first = True)\n",
    "        self.rnn_beta = nn.GRU(embedding_dimension, embedding_dimension, batch_first = True)\n",
    "        self.alpha_attention = AlphaAttention(embedding_dimension)\n",
    "        self.beta_attention = BetaAttention(embedding_dimension)\n",
    "        self.fully_connected = nn.Linear(embedding_dimension, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, reverse_x, reverse_masks):\n",
    "        reverse_x = self.embedding(reverse_x)\n",
    "        reverse_x = SumEmbeddingsWithMask(reverse_x, reverse_masks)\n",
    "        gi, _ = self.rnn_alpha(reverse_x)\n",
    "        hi, _ = self.rnn_beta(reverse_x)\n",
    "        alpha = self.alpha_attention(gi)\n",
    "        beta = self.beta_attention(hi)\n",
    "        context = AttentionSum(alpha, beta, reverse_x, reverse_masks)\n",
    "        logits = self.fully_connected(context)\n",
    "        probability = self.sigmoid(logits).squeeze()\n",
    "        \n",
    "        return probability\n",
    "\n",
    "retain = Retain(diagnosis_codes = len(#******************** not implemented yet. part of data loader))\n",
    "retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain evaluation\n",
    "\n",
    "def eval(model, value_loader):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, reverse_x, reverse_masks, y in value_loader:\n",
    "        y_logit = model(x, masks, reverse_x, reverse_masks)\n",
    "        y_hat = y_logit > 0.5\n",
    "        \n",
    "        y_score = torch.cat((y_score, y_logit.detach().to('cpu')), dim = 0)\n",
    "        y_pred = torch.cat((y_pred, y_hat.detach().to('cpu')), dim = 0)\n",
    "        y_true torch.cat((y_true, y.detach().to('cpu')), dim = 0)\n",
    "        \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average = 'binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain training\n",
    "\n",
    "def train(model, train_loader, value_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "        for x, masks, reverse_x, reverse_masks, y in traing_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, reverse_x, reverse_masks)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "        training_loss = training_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval(model, value_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        \n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to main function at the end\n",
    "\n",
    "#this line is redundant? may remove as needed or remove the other line above.\n",
    "retain = Retain(diagnosis_codes = len(#***************************not implemented yet))\n",
    "    \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr = 1e-3)\n",
    "    \n",
    "epochs = 10\n",
    "train(retain, train_loader, value_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35463316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model (The most important model to finish)\n",
    "\n",
    "#Source: https://github.com/subendhu19/CLOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clout outline photos to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, epochs=5, batchsize=50, vocabsize=5, embsize=100):\n",
    "        super(RNN, self).__init__()\n",
    "        self.epochs = 5\n",
    "        self.batchsize = batchsize\n",
    "        self.vocabsize = vocabsize\n",
    "        self.embsize = embsize\n",
    "\n",
    "        self.emb_icd = nn.Linear(vocabsize_icd, embsize_icd)\n",
    "        self.emb_meds = nn.Linear(vocabsize_meds, embsize_meds)\n",
    "        self.emb_labs = nn.Linear(vocabsize_labs, embsize_labs)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embsize, hidden_size=embsize, num_layers=1)\n",
    "        self.out = nn.Linear(embsize, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_icd, input_med, input_lab, input_latent, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(input_icd)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        input_icd = self.emb_icd(input_icd)\n",
    "        input_med = self.emb_meds(input_med)\n",
    "        input_lab = self.emb_labs(input_lab)\n",
    "\n",
    "        inputs = F.relu(torch.cat((input_icd, input_med, input_lab, input_latent),1))\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0],1,inputs.size()[1])\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs.squeeze(), hidden\n",
    "\n",
    "    def predict(self, input_icd, input_med, input_lab, input_latent):\n",
    "        out, hid = self.forward(input_icd, input_med, input_lab, input_latent, None)\n",
    "        return self.sig(out[-1]).data[0]\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Coq tactic prediction')\n",
    "parser.add_argument('--emb_weights', type=str, default='../full data/CAE/CAE_embedding_weights.npy',\n",
    "                    help='location of the embedding weight file (default: ../full data/CAE/CAE_embedding_weights.npy)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "n_epochs = 1\n",
    "vocabsize_icd = 942\n",
    "vocabsize_meds = 3202\n",
    "vocabsize_labs = 284 #all 681\n",
    "vocabsize = vocabsize_icd+vocabsize_meds+vocabsize_labs\n",
    "\n",
    "embsize_icd = 50\n",
    "embsize_meds = 75\n",
    "embsize_labs = 50\n",
    "embsize_latent = 175\n",
    "embsize = embsize_icd + embsize_labs + embsize_meds + embsize_latent\n",
    "\n",
    "input_seqs_icd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.3digitICD9.seqs')))\n",
    "input_seqs_meds = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.meds.seqs')))\n",
    "input_seqs_labs = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.abnlabs.seqs')))\n",
    "# input_seqs_latent = np.array(pickle.load(open('../full data/CAE/Embeddings.seqs')))\n",
    "latent_weights = pickle.load(open(args.emb_weights))\n",
    "\n",
    "input_seqs_fullicd = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.seqs')))\n",
    "\n",
    "icditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.types', 'rb'))\n",
    "meditems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.meds.types', 'rb'))\n",
    "labitems = pickle.load(open('../full data/type dictionaries/MIMICIIIPROCESSED.abnlabs.types', 'rb'))\n",
    "\n",
    "model_name = 'AE'\n",
    "if args.emb_weights == '../full data/CAE/CAE_embedding_weights.npy':\n",
    "    model_name = 'CAE'\n",
    "\n",
    "interpretation_file = open(\"RNN_CLatent_Interpretations_\" + model_name + \".txt\", 'w')\n",
    "\n",
    "# overall_risk_factor_file = open(\"risk_factors_averaged.txt\", \"w\")\n",
    "\n",
    "labnames = {}\n",
    "lab_dict_file = open('D_LABITEMS.csv', 'r')\n",
    "lab_dict_file.readline()\n",
    "for line in lab_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    labnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "lab_dict_file.close()\n",
    "\n",
    "icdnames = {}\n",
    "icd_dict_file = open('D_ICD_DIAGNOSES.csv', 'r')\n",
    "icd_dict_file.readline()\n",
    "for line in icd_dict_file:\n",
    "    tokens = line.strip().split(',')\n",
    "    icdnames[tokens[1].replace('\"','')] = tokens[2]\n",
    "icd_dict_file.close()\n",
    "\n",
    "icd_scores = {}\n",
    "med_scores = {}\n",
    "lab_scores = {}\n",
    "\n",
    "icd_totals = {}\n",
    "med_totals = {}\n",
    "lab_totals = {}\n",
    "\n",
    "def get_ICD(icd):\n",
    "    ret_str = \"\"\n",
    "    icd_str = icditems.keys()[icditems.values().index(icd)]\n",
    "    actual_key = icd_str.replace(\".\", \"\")[2:]\n",
    "    if actual_key in icdnames:\n",
    "        ret_str = icdnames[actual_key]\n",
    "    else:\n",
    "        ret_str = icd_str\n",
    "    return ret_str\n",
    "\n",
    "def get_med(med):\n",
    "    ret_str = meditems.keys()[meditems.values().index(med)]\n",
    "    return ret_str\n",
    "\n",
    "def get_lab(lab):\n",
    "    ret_str = labnames[labitems.keys()[labitems.values().index(lab)]]\n",
    "    return ret_str\n",
    "\n",
    "print 'Data loaded..'\n",
    "\n",
    "labels = np.array(pickle.load(open('../full data/MIMICIIIPROCESSED.morts')))\n",
    "\n",
    "trainratio = 0.7\n",
    "validratio = 0.1\n",
    "testratio = 0.2\n",
    "\n",
    "trainlindex = int(len(input_seqs_icd)*trainratio)\n",
    "validlindex = int(len(input_seqs_icd)*(trainratio + validratio))\n",
    "\n",
    "print 'Data prepared..'\n",
    "\n",
    "def convert_to_one_hot(code_seqs, len):\n",
    "    new_code_seqs = []\n",
    "    for code_seq in code_seqs:\n",
    "        one_hot_vec = np.zeros(len)\n",
    "        for code in code_seq:\n",
    "            one_hot_vec[code] = 1\n",
    "        new_code_seqs.append(one_hot_vec)\n",
    "    return np.array(new_code_seqs)\n",
    "\n",
    "def get_avg(seqs, type):\n",
    "    count = 0\n",
    "    for seq in seqs:\n",
    "        count += len(seq)\n",
    "    val = round(count*1.0/len(seqs))\n",
    "    if type == 'i':\n",
    "        return min(4, int(val/5))\n",
    "    else:\n",
    "        return min(4, int(val/50))\n",
    "\n",
    "def get_factors(icd_seq, med_seq, lab_seq, model, actual_score, full_icd):\n",
    "    potential_test_data = []\n",
    "\n",
    "    for seq in range(len(icd_seq)):\n",
    "        for i in range(len(icd_seq[seq])):\n",
    "            potential_test_data.append((\"icd\", full_icd[seq][i], seq, icd_seq[:seq]+[icd_seq[seq][:i] + icd_seq[seq][i+1:]]+icd_seq[seq+1:], med_seq, lab_seq))\n",
    "    for seq in range(len(med_seq)):\n",
    "        for i in range(len(med_seq[seq])):\n",
    "            potential_test_data.append((\"med\", med_seq[seq][i], seq, icd_seq, med_seq[:seq]+[med_seq[seq][:i]+med_seq[seq][i+1:]]+med_seq[seq+1:], lab_seq))\n",
    "    for seq in range(len(lab_seq)):\n",
    "        for i in range(len(lab_seq[seq])):\n",
    "            potential_test_data.append((\"lab\", lab_seq[seq][i], seq, icd_seq, med_seq, lab_seq[:seq]+[lab_seq[seq][:i] + lab_seq[seq][i+1:]]+lab_seq[seq+1:]))\n",
    "\n",
    "    risk_scores = []\n",
    "\n",
    "    for pt in potential_test_data:\n",
    "        test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(pt[3], vocabsize_icd)).float())\n",
    "        test_input_med = Variable(torch.from_numpy(convert_to_one_hot(pt[4], vocabsize_meds)).float())\n",
    "        test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(pt[5], vocabsize_labs)).float())\n",
    "\n",
    "        latent_inputs_oh = np.concatenate((convert_to_one_hot(pt[3], vocabsize_icd), convert_to_one_hot(pt[4], vocabsize_meds), convert_to_one_hot(pt[5], vocabsize_labs)), 1)\n",
    "        latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "        latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "        factor_score = actual_score - model.predict(test_input_icd, test_input_med, test_input_lab, latent_inputs)\n",
    "        factor = \"\"\n",
    "        if pt[0] == 'icd':\n",
    "            icd_tag = get_ICD(pt[1])\n",
    "            factor = \"ICD-\"+icd_tag\n",
    "            if icd_tag in icd_scores:\n",
    "                icd_scores[icd_tag] += factor_score\n",
    "                icd_totals[icd_tag] += 1\n",
    "            else:\n",
    "                icd_scores[icd_tag] = factor_score\n",
    "                icd_totals[icd_tag] = 1\n",
    "        elif pt[0] == 'med':\n",
    "            med_tag = get_med(pt[1])\n",
    "            factor = \"Med-\"+med_tag\n",
    "            if med_tag in med_scores:\n",
    "                med_scores[med_tag] += factor_score\n",
    "                med_totals[med_tag] += 1\n",
    "            else:\n",
    "                med_scores[med_tag] = factor_score\n",
    "                med_totals[med_tag] = 1\n",
    "        else:\n",
    "            lab_tag = get_lab(pt[1])\n",
    "            factor = \"Lab-\"+lab_tag\n",
    "            if lab_tag in lab_scores:\n",
    "                lab_scores[lab_tag] += factor_score\n",
    "                lab_totals[lab_tag] += 1\n",
    "            else:\n",
    "                lab_scores[lab_tag] = factor_score\n",
    "                lab_totals[lab_tag] = 1\n",
    "        risk_scores.append((\"Encounter-\"+str(pt[2])+\": \"+factor, factor_score))\n",
    "\n",
    "    risk_scores.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    return risk_scores[:10]\n",
    "\n",
    "print 'Starting training..'\n",
    "\n",
    "batchsize = 50\n",
    "\n",
    "# ICD_wise_tot_tr = np.zeros(5)\n",
    "# meds_wise_tot_tr = np.zeros(5)\n",
    "# labs_wise_tot_tr = np.zeros(5)\n",
    "\n",
    "# for i in range(len(train_input_seqs_icd)):\n",
    "# \tICD_wise_tot_tr[get_avg(train_input_seqs_icd[i], 'i')] += 1\n",
    "# \tmeds_wise_tot_tr[get_avg(train_input_seqs_meds[i], 'm')] += 1\n",
    "# \tlabs_wise_tot_tr[get_avg(train_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "# print 'ICD-wise train total', ICD_wise_tot_tr\n",
    "# print 'Meds-wise train total', meds_wise_tot_tr\n",
    "# print 'Labs-wise train total', labs_wise_tot_tr\n",
    "\n",
    "best_aucrocs = []\n",
    "for run in range(10):\n",
    "    print 'Run', run\n",
    "\n",
    "    perm = np.random.permutation(input_seqs_icd.shape[0])\n",
    "    rinput_seqs_icd = input_seqs_icd#[perm]\n",
    "    rinput_seqs_meds = input_seqs_meds#[perm]\n",
    "    rinput_seqs_labs = input_seqs_labs#[perm]\n",
    "    # rinput_seqs_latent = input_seqs_latent[perm]\n",
    "    rinput_seqs_fullicd = input_seqs_fullicd#[perm]\n",
    "    rlabels = labels#[perm]\n",
    "\n",
    "    train_input_seqs_icd = rinput_seqs_icd[:trainlindex]\n",
    "    train_input_seqs_meds = rinput_seqs_meds[:trainlindex]\n",
    "    train_input_seqs_labs = rinput_seqs_labs[:trainlindex]\n",
    "    # train_input_seqs_latent = rinput_seqs_latent[:trainlindex]\n",
    "    train_labels = rlabels[:trainlindex]\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "    valid_input_seqs_icd = rinput_seqs_icd[trainlindex:validlindex]\n",
    "    valid_input_seqs_meds = rinput_seqs_meds[trainlindex:validlindex]\n",
    "    valid_input_seqs_labs = rinput_seqs_labs[trainlindex:validlindex]\n",
    "    # valid_input_seqs_latent = rinput_seqs_latent[trainlindex:validlindex]\n",
    "    valid_labels = rlabels[trainlindex:validlindex]\n",
    "\n",
    "    test_input_seqs_icd = rinput_seqs_icd[validlindex:]\n",
    "    test_input_seqs_meds = rinput_seqs_meds[validlindex:]\n",
    "    test_input_seqs_labs = rinput_seqs_labs[validlindex:]\n",
    "    # test_input_seqs_latent = rinput_seqs_latent[validlindex:]\n",
    "    test_input_seqs_fullicd = rinput_seqs_fullicd[validlindex:]\n",
    "    test_labels = rlabels[validlindex:]\n",
    "\n",
    "    n_iters = train_input_seqs_icd.shape[0]\n",
    "\n",
    "    model = RNN(n_epochs, 1, vocabsize, embsize)\n",
    "    criterion = nn.BCEWithLogitsLoss(size_average=False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    aucrocs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        print 'Epoch', (epoch+1)\n",
    "\n",
    "        for i in (range(0, n_iters, batchsize)):\n",
    "            batch_icd = train_input_seqs_icd[i:i+batchsize]\n",
    "            batch_meds = train_input_seqs_meds[i:i+batchsize]\n",
    "            batch_labs = train_input_seqs_labs[i:i+batchsize]\n",
    "            # batch_latent = train_input_seqs_latent[i:i+batchsize]\n",
    "\n",
    "            batch_train_labels = train_labels[i:i+batchsize]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses = []\n",
    "\n",
    "            for i in range(len(batch_icd)):\n",
    "                icd_onehot = convert_to_one_hot(batch_icd[i], vocabsize_icd)\n",
    "                med_onehot = convert_to_one_hot(batch_meds[i], vocabsize_meds)\n",
    "                lab_onehot = convert_to_one_hot(batch_labs[i], vocabsize_labs)\n",
    "\n",
    "                icd_inputs = Variable(torch.from_numpy(icd_onehot).float())\n",
    "                med_inputs = Variable(torch.from_numpy(med_onehot).float())\n",
    "                lab_inputs = Variable(torch.from_numpy(lab_onehot).float())\n",
    "\n",
    "                latent_inputs_oh = np.concatenate((icd_onehot, med_onehot, lab_onehot), 1)\n",
    "                latent_inputs = np.dot(latent_inputs_oh, latent_weights)\n",
    "                latent_inputs = Variable(torch.from_numpy(latent_inputs).float())\n",
    "\n",
    "                # latent_inputs = Variable(torch.from_numpy(np.array(batch_latent[iter])).float())\n",
    "\n",
    "                targets = Variable(torch.from_numpy(batch_train_labels[i]).float())\n",
    "\n",
    "                # Use teacher forcing 50% of the time\n",
    "                force = random.random() < 0.5\n",
    "                outputs, hidden = model(icd_inputs, med_inputs, lab_inputs, latent_inputs, None, force)\n",
    "\n",
    "                #print outputs[-1], targets\n",
    "                losses.append(criterion(outputs[-1], targets))\n",
    "\n",
    "            loss = sum(losses)/len(batch_icd)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "\n",
    "        #print(epoch, epoch_loss)\n",
    "\n",
    "        ## Validation phase\n",
    "        vpredictions = np.zeros(len(valid_input_seqs_icd))\n",
    "        for i in range(len(valid_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(valid_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(valid_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(valid_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(valid_input_seqs_latent[i])).float())\n",
    "            vpredictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "\n",
    "        print \"Validation AUC_ROC: \", roc_auc_score(valid_labels, vpredictions)\n",
    "\n",
    "        ## Testing phase\n",
    "        predictions = np.zeros(len(test_input_seqs_icd))\n",
    "\n",
    "        # ICD_wise_corr = np.zeros(5)\n",
    "        # meds_wise_corr = np.zeros(5)\n",
    "        # labs_wise_corr = np.zeros(5)\n",
    "        # ICD_wise_tot = np.zeros(5)\n",
    "        # meds_wise_tot = np.zeros(5)\n",
    "        # labs_wise_tot = np.zeros(5)\n",
    "\n",
    "        for i in range(len(test_input_seqs_icd)):\n",
    "            test_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "            test_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "            test_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "\n",
    "            test_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "            test_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "            test_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "            # test_input_latent = Variable(torch.from_numpy(np.array(test_input_seqs_latent[i])).float())\n",
    "            predictions[i] = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "            \n",
    "            # ICD_wise_corr[get_avg(test_input_seqs_icd[i], 'i')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # ICD_wise_tot[get_avg(test_input_seqs_icd[i], 'i')] += 1\n",
    "\n",
    "            # meds_wise_corr[get_avg(test_input_seqs_meds[i], 'm')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # meds_wise_tot[get_avg(test_input_seqs_meds[i], 'm')] += 1\n",
    "\n",
    "            # labs_wise_corr[get_avg(test_input_seqs_labs[i], 'l')] += int((predictions[i]>0.5)*1 == test_labels[i])\n",
    "            # labs_wise_tot[get_avg(test_input_seqs_labs[i], 'l')] += 1\n",
    "\n",
    "        print \"Test AUC_ROC: \", roc_auc_score(test_labels, predictions)\n",
    "\n",
    "        aucrocs.append(roc_auc_score(test_labels, predictions))\n",
    "        #fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "        #pickle.dump({\"FPR\":fpr, \"TPR\":tpr}, open('roc_clout_cornn.p', 'wb'))\n",
    "        #actual_predictions = (predictions>0.5)*1\n",
    "        #print classification_report(test_labels, actual_predictions)\n",
    "\n",
    "    best_aucrocs.append(max(aucrocs))\n",
    "\n",
    "print \"Average AUCROC:\", np.mean(best_aucrocs), \"+/-\", np.std(best_aucrocs) \n",
    "\n",
    "# print \"Final testing and interpretations\"\n",
    "# predictions = np.zeros(len(test_input_seqs_icd))\n",
    "# for i in (range(len(test_input_seqs_icd))):\n",
    "# \ttest_input_icd = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd)).float())\n",
    "# \ttest_input_med = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds)).float())\n",
    "# \ttest_input_lab = Variable(torch.from_numpy(convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)).float())\n",
    "# \ttest_input_latent_oh = np.concatenate((convert_to_one_hot(test_input_seqs_icd[i], vocabsize_icd), convert_to_one_hot(test_input_seqs_meds[i], vocabsize_meds), convert_to_one_hot(test_input_seqs_labs[i], vocabsize_labs)), 1)\n",
    "# \ttest_input_latent = np.dot(test_input_latent_oh, latent_weights)\n",
    "# \ttest_input_latent = Variable(torch.from_numpy(test_input_latent).float())\n",
    "\n",
    "# \ttest_score = model.predict(test_input_icd, test_input_med, test_input_lab, test_input_latent)\n",
    "# \tpredictions[i] = test_score\n",
    "# \ttop_risk_factors = get_factors(test_input_seqs_icd[i], test_input_seqs_meds[i], test_input_seqs_labs[i], model, test_score, test_input_seqs_fullicd[i]) \n",
    "# \tif (test_score>0.5):\n",
    "# \t\tinterpretation_file.write(\"ID: \" + str(i) + \" True label: \"+str(test_labels[i])+\"\\n\")\n",
    "# \t\tfor rf in top_risk_factors:\n",
    "# \t\t\tinterpretation_file.write(str(rf)+\"\\n\")\n",
    "# \t\tinterpretation_file.write(\"\\n\")\n",
    "\n",
    "interpretation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80438e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clout evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Graphs etc...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
